{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                            Available Models                            </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                      </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-base-ft  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-large-ft </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-base     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-large    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "└────────────────┴───────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                            Available Models                            \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-base-ft \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-large-ft\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-base    \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-large   \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴───────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"florence-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-09 17:36:38.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: microsoft/Florence-2-large-ft\u001b[0m\n",
      "\u001b[32m2024-11-09 17:36:38.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-11-09 17:36:38.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: float16\u001b[0m\n",
      "/home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"microsoft/Florence-2-large-ft\", device=\"cuda\", dtype=\"float16\")\n",
    "\n",
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<CAPTION>\"\n",
    "model.infer(image, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='A woman with glasses is gesturing with both hands.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 40\n",
    "model.infer_batch([image] * batch_size, [prompt] * batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text='In this image I can see a woman wearing green color dress and spectacles. Background is in black color.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<DETAILED_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text='A woman is standing in front of a white wall. She is wearing a green shirt and has bracelets on her wrists. The woman has glasses on and her hair is long and brown. ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text={'bboxes': [], 'labels': []})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text={'bboxes': [[325.1200256347656, 221.33999633789062, 468.4800109863281, 268.94000244140625], [337.40802001953125, 179.1800079345703, 466.4320373535156, 340.3399963378906], [237.05601501464844, 124.0999984741211, 632.3200073242188, 678.97998046875]], 'labels': ['glasses', 'human face', 'woman']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<OD>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text={'bboxes': [[221.69601440429688, 124.0999984741211, 643.5840454101562, 678.97998046875], [337.40802001953125, 179.86000061035156, 466.4320373535156, 340.3399963378906], [325.1200256347656, 221.33999633789062, 468.4800109863281, 268.94000244140625]], 'labels': ['woman in green shirt with glasses on stage', 'human face', 'glasses']})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text={'bboxes': [[237.05601501464844, 124.0999984741211, 632.3200073242188, 678.97998046875], [299.52001953125, 126.81999969482422, 501.2480163574219, 343.05999755859375], [338.4320068359375, 179.1800079345703, 467.4560241699219, 340.3399963378906], [505.3440246582031, 542.97998046875, 612.864013671875, 621.1799926757812], [345.6000061035156, 557.9400024414062, 461.31201171875, 627.97998046875], [325.1200256347656, 220.66000366210938, 468.4800109863281, 268.94000244140625], [390.656005859375, 241.05999755859375, 429.5680236816406, 281.8600158691406], [390.656005859375, 289.3399963378906, 440.83203125, 308.3800048828125]], 'labels': ['', '', '', '', '', '', '', '']})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<REGION_PROPOSAL>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"1000\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
