{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-09 17:47:21.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: fancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\n",
      "\u001b[32m2024-11-09 17:47:21.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-11-09 17:47:21.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: bfloat16\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267db39461f24b92923316d274862111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"fancyfeast/llama-joycaption-alpha-two-hf-llava\", device=\"cuda\", dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text='This image is a digital photograph featuring two men conversing, sitting on a seamless, white surface that extends horizontally, dividing the background evenly into two parts: white above and a minimalist, simple white gray below, mimicking a split-screen effect. The man to the left is wearing a light blue blazer over a blue-checked dress shirt, paired with tan slacks and black shoes. He has a medium build, dark hair, and a full beard with glasses. His mouth is slightly open as if')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"https://t4.ftcdn.net/jpg/02/93/24/11/360_F_293241113_zlhCKkr1XL8gpSTobEfectLmx0sL5hUu.jpg\"\n",
    "prompt = \"Describe the image in detail.\"\n",
    "model.infer(image, prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  Model Info                                  </span>\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute                 </span>│<span style=\"font-weight: bold\"> Value                                          </span>│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Model ID                  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Device                    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> cuda                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Dtype                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> torch.bfloat16                                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Number of Inferences      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 1                                              </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Inference Time (ms) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 4024.8920                                      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Latency (ms)      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 4024.8920                                      </span>│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  Model Info                                  \u001b[0m\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                         \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│\u001b[36m \u001b[0m\u001b[36mModel ID                 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDevice                   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mcuda                                          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDtype                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtorch.bfloat16                                \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mNumber of Inferences     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1                                             \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Inference Time (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m4024.8920                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Latency (ms)     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m4024.8920                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(categories=None, boxes=None, masks=None, poses=None, text='This is a studio photograph of two middle-aged men seated against a stark white background. The image is split horizontally with the bottom half featuring a seamless grey gradient, blending smoothly from white at the top to grey at the bottom. Both men are dressed in casual business attire, each sitting on the borderline between the white and grey sections. \\n\\nThe man on the left has a full beard and short dark hair. He is wearing a light blue blazer over a light blue striped shirt, matched with beige chinos and brown oxford shoes. His stance is casual, and he is looking at the person on the right, who has a clean-shaven face and short dark hair. This man is wearing a light blue short-sleeve button-up shirt, black pants, and grey slip-on shoes. He is also conversing, gesturing slightly with his right hand.\\n\\nThe background and furniture suggest a formal or professional setting, potentially for a workplace presentation or business discussion. There are no other objects in the background, making the focus solely on the two men. The lighting is bright and even, highlighting their expressions and attire clearly. This photograph is likely taken in a professional studio, designed to provide a crisp, clean presentation.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text=\"This is a photograph of two men sitting side by side, separated by a thin white dividing line on a blank white background. Both men appear to be engaged in a casual conversation. The man on the left has short, dark brown hair and a full beard. He wears glasses with dark frames, a light blue button-up shirt underneath a brown blazer, and beige pants paired with dark brown shoes. He holds his hands loosely folded in front of him. The man on the right has short, light brown hair and is clean-shaven. He is dressed in a light gray button-up shirt and black pants, finished with casual blue sneakers. He leans slightly towards the other man, gesturing open-palmed towards him as if making a point or engaging in the conversation. Both men's facial expressions suggest they are involved and attentive to each other. The background and the thin dividing line emphasize a minimalistic and clean aesthetic, drawing attention primarily to the interaction between the two men.\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "images = [image] * batch_size\n",
    "prompts = [prompt] * batch_size\n",
    "\n",
    "model.infer_batch(images, prompts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.81 s, sys: 45.6 ms, total: 7.86 s\n",
      "Wall time: 7.64 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(categories=None, boxes=None, masks=None, poses=None, text='This is a photograph featuring two men seated on a white, minimalist platform against a stark white background. The man on the left is wearing a blue blazer over a light blue button-up shirt, paired with beige chinos and black dress shoes. He has a full beard, dark brown hair, and wears square-framed glasses. He holds his hands together on his lap and has a contemplative expression.\\n\\nThe man on the right is dressed in a casual attire with a light blue short-sleeve')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.infer(images[0], prompt, max_new_tokens=100)\n",
    "model.infer(images[1], prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.16 s, sys: 42.7 ms, total: 4.2 s\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(categories=None, boxes=None, masks=None, poses=None, text='The image is a photograph of two men sitting on the edge of a transparent white platform against a light grey background. Both men appear to be engaged in conversation, expressing themselves with animated body language.\\n\\nOn the left is a man with a dark beard and mustache, dressed in a light blue button-up shirt with rolled-up sleeves, dark brown pants, and beige loafers. He is wearing a grey blazer over his shirt, crossed formally at the cuffs, and has his hands clasped together.'),\n",
       " Result(categories=None, boxes=None, masks=None, poses=None, text='The image is a digital photograph featuring two men seated on a light-gray ledge in the horizontal center. The background is plain and white, accentuating the men and their expressions. On the left, a man with a thick brown beard and mustache, glasses, and curly dark hair sits with brown leather shoes. He wears a light blue button-up shirt under a light gray blazer paired with beige trousers. His legs are crossed, and he has his hands clasped. His expression is contemplative,')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.infer_batch(images, prompts, max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
