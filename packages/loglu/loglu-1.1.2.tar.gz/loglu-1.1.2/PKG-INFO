Metadata-Version: 2.1
Name: loglu
Version: 1.1.2
Summary: The Logarithmic Linear Unit (LogLU) is a novel activation function designed for deep neural networks, improving convergence speed, stability, and overall model performance
Author: Rishi Chaitanya Sri Prasad Nalluri
Author-email: rishichaitanya888@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tensorflow>=2.10.0
Requires-Dist: keras>=2.10.0


## LogLU (Logarithmic Linear Units) — Activation Function

Welcome to **LogLU**, a novel activation function designed to enhance the performance of deep neural networks.

The **Logarithmic Linear Unit (LogLU)** improves convergence speed, stability, and overall model performance. Whether you're working on AI for image recognition, NLP, or other applications, this activation function is designed to make your models more efficient.

### Why LogLU?

Activation functions like ReLU are commonly used, but sometimes a more refined and efficient solution is required. Here’s why LogLU stands out:

- **Faster Convergence**: LogLU helps your models train more quickly, saving time and computational resources.
- **Stability**: It prevents issues like exploding or vanishing gradients, ensuring smooth training.
- **Performance**: LogLU consistently improves accuracy and reduces loss compared to traditional activation functions.

### Installation

To use LogLU, simply install it using the following command:

```bash
pip install loglu
```

### Usage

Here’s how to integrate LogLU into your deep learning models.

#### With TensorFlow:

```python
import tensorflow as tf
from loglu import LogLU # Importing the LogLU activation function from the loglu package

# Define a model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, input_shape=(784,)),
    tf.keras.layers.Activation(LogLU()),  # Use LogLU as the activation function
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Summary of the model
model.summary()
```

### How It Works

LogLU combines the smooth characteristics of logarithmic functions with the simplicity of linear functions like ReLU. It maintains gradient flow even in deep networks, ensuring fast and stable learning without the typical issues of gradient vanishing or exploding.

For those interested in the mathematical details:
- If `x > 0`: LogLU behaves as a linear function.
- If `x <= 0`: It adopts a logarithmic form, providing a smoother handling of negative values.

### Feedback, Bugs, & Contributions

If you have feedback or encounter any bugs, please feel free to open an issue or contribute on GitHub:

- GitHub: [https://github.com/Rishichaitanya-Nalluri/LogLU](https://github.com/Rishichaitanya-Nalluri/LogLU)
- Email: rishichaitanya888@gmail.com

Let’s collaborate to make machine learning faster and more efficient!

---

Happy coding and may your models always converge smoothly!


## Patent Notice

This software implements methods covered by Indian Published Patent Application No. 202341087256, currently published but not yet granted. Use of the patented methods may require a separate license. Please refer to the LICENSE file for more details.
