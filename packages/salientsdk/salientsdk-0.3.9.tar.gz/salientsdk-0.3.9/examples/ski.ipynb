{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salient Predictions Ski-Cast\n",
    "\n",
    "In November, Salient predicts snow accumulation at 90 IKON and Epic resorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f129f00cf90>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "force = False\n",
    "\n",
    "year = datetime.datetime.now().year\n",
    "today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "start_month = 10  # first day of the ski season\n",
    "end_month = 5  # final day of the ski season\n",
    "fcst_date = f\"{year}-10-15\"\n",
    "hist_start = f\"1990-{start_month}-01\"\n",
    "fcst_end = f\"{year+1}-{end_month}-01\"\n",
    "vars = [\"temp\", \"precip\"]\n",
    "\n",
    "\n",
    "sk.set_file_destination(\"ski_example\")\n",
    "sk.login(\"username\", \"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "resorts = {\n",
    "    \"japan\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": 137.861, \"lat\": 36.690, \"name\": \"Hakuba\"},  # epic\n",
    "            {\"lon\": 140.687, \"lat\": 42.824, \"name\": \"Rusutsu\"},  # epic\n",
    "            {\"lon\": 140.685, \"lat\": 42.824, \"name\": \"Niseko\"},  # ikon\n",
    "            {\"lon\": 140.685, \"lat\": 42.824, \"name\": \"Lotte Arai\"},  # ikon\n",
    "            # Tazawako indy\n",
    "            # Okunakayama\n",
    "        ]\n",
    "    ),\n",
    "    \"alps\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": 6.8632749, \"lat\": 45.924065, \"name\": \"Chamonix\"},  # ikon\n",
    "            {\"lon\": 7.7522747, \"lat\": 46.0222204, \"name\": \"Zermatt\"},  # ikon\n",
    "            {\"lon\": 8.5916293, \"lat\": 46.6324621, \"name\": \"Andermatt-Sedrun\"},  # epic\n",
    "            {\"lon\": 12.3925407, \"lat\": 47.4492375, \"name\": \"Kitzbuhel\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"pnw\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -123.204545, \"lat\": 49.396018, \"name\": \"Cypress\"},  # ikon\n",
    "            {\"lon\": -121.6781891, \"lat\": 44.0024701, \"name\": \"Bachelor\"},  # ikon\n",
    "            {\"lon\": -121.0890197, \"lat\": 47.7448119, \"name\": \"Stevens Pass\"},  # epic\n",
    "            {\"lon\": -122.9486474, \"lat\": 50.1149639, \"name\": \"Whistler\"},  # epic\n",
    "            {\"lon\": -121.4747533, \"lat\": 46.9352963, \"name\": \"Crystal Mtn\"},  # ikon\n",
    "            {\"lon\": -121.4257485, \"lat\": 47.4442426, \"name\": \"Alpental\"},  # ikon\n",
    "            {\"lon\": -121.4164161, \"lat\": 47.4245711, \"name\": \"Snoqualmie\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"rockies\": pd.DataFrame(\n",
    "        [\n",
    "            # The four Aspen resorts all perform similarly.  Combine into one:\n",
    "            # {\"lon\": -106.9490961, \"lat\": 39.2083984, \"name\": \"Aspen Snowmass\"},\n",
    "            # {\"lon\": -106.8610687, \"lat\": 39.2058029, \"name\": \"Buttermilk\"},\n",
    "            # {\"lon\": -106.8553613, \"lat\": 39.1824124, \"name\": \"Aspen Highlands\"},\n",
    "            {\"lon\": -106.818227, \"lat\": 39.1862601, \"name\": \"Aspen Mtn\"},  # ikon\n",
    "            {\"lon\": -106.8045169, \"lat\": 40.4571991, \"name\": \"Steamboat\"},  # ikon\n",
    "            # Beaver Creek and Vail perform similarly and are right next to each other\n",
    "            # {\"lon\": -106.5167109, \"lat\": 39.6042863, \"name\": \"Beaver Creek\"}, # epic\n",
    "            {\"lon\": -106.3549717, \"lat\": 39.6061444, \"name\": \"Vail\"},  # epic\n",
    "            {\"lon\": -106.1516265, \"lat\": 39.501419, \"name\": \"Copper\"},  # ikon\n",
    "            {\"lon\": -106.0676088, \"lat\": 39.4808351, \"name\": \"Breckenridge\"},  # epic\n",
    "            {\"lon\": -105.9437656, \"lat\": 39.6075962, \"name\": \"Keystone\"},  # epic\n",
    "            {\"lon\": -105.8719397, \"lat\": 39.6425118, \"name\": \"A-Basin\"},  # ikon\n",
    "            {\"lon\": -105.762488, \"lat\": 39.8868392, \"name\": \"Winter Park\"},  # ikon\n",
    "            {\"lon\": -105.5826786, \"lat\": 39.9372203, \"name\": \"Eldora\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"new_england\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -72.9278443, \"lat\": 43.0906207, \"name\": \"Stratton\"},  # ikon\n",
    "            {\"lon\": -72.9204014, \"lat\": 42.9602444, \"name\": \"Mt Snow\"},  # epic\n",
    "            {\"lon\": -72.8944139, \"lat\": 44.1359019, \"name\": \"Sugarbush\"},  # ikon\n",
    "            # {\"lon\": -72.842512, \"lat\": 43.6621499, \"name\": \"Pico\"}, # ikon near Killington\n",
    "            {\"lon\": -72.7967531, \"lat\": 43.6262922, \"name\": \"Killington\"},  # ikon\n",
    "            {\"lon\": -72.7814124, \"lat\": 44.5303066, \"name\": \"Stowe\"},  # epic\n",
    "            {\"lon\": -72.7170416, \"lat\": 43.4018257, \"name\": \"Okemo\"},  # epic\n",
    "            {\"lon\": -72.08014, \"lat\": 43.331889, \"name\": \"Sunapee\"},  # epic\n",
    "            {\"lon\": -71.8655176, \"lat\": 43.0198715, \"name\": \"Crotched\"},  # epic\n",
    "            {\"lon\": -71.6336041, \"lat\": 44.0563456, \"name\": \"Loon\"},  # ikon\n",
    "            {\"lon\": -71.2393036, \"lat\": 44.2640724, \"name\": \"Wildcat\"},  # epic\n",
    "            {\"lon\": -71.229443, \"lat\": 44.082771, \"name\": \"Attitash\"},  # epic\n",
    "            {\"lon\": -70.8568727, \"lat\": 44.4734182, \"name\": \"Sunday River\"},  # ikon\n",
    "            {\"lon\": -70.3085109, \"lat\": 45.0541811, \"name\": \"Sugarloaf\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"europe\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": 1.4707674, \"lat\": 42.5729217, \"name\": \"Arinsal\"},  # ikon\n",
    "            {\"lon\": 1.499825, \"lat\": 42.6317345, \"name\": \"Ordino Arcal√≠s\"},  # ikon\n",
    "            {\"lon\": 1.6462281, \"lat\": 42.5783833, \"name\": \"Grandvalira\"},  # ikon\n",
    "            {\"lon\": 11.6520936, \"lat\": 46.5739752, \"name\": \"Dolomiti\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"na_west\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -120.2483913, \"lat\": 39.1906091, \"name\": \"Palisades Tahoe\"},  # ikon\n",
    "            {\"lon\": -120.1210934, \"lat\": 39.2745678, \"name\": \"Northstar\"},  # epic\n",
    "            {\"lon\": -120.0651665, \"lat\": 38.6847514, \"name\": \"Kirkwood\"},  # epic\n",
    "            {\"lon\": -119.9428424, \"lat\": 38.9569241, \"name\": \"Heavenly\"},  # epic\n",
    "            {\"lon\": -119.8859331, \"lat\": 50.8844311, \"name\": \"Sun Peaks\"},  # ikon\n",
    "            {\"lon\": -119.0906293, \"lat\": 37.7679169, \"name\": \"June\"},  # ikon\n",
    "            {\"lon\": -119.0267806, \"lat\": 37.6510972, \"name\": \"Mammoth\"},  # ikon\n",
    "            {\"lon\": -118.1630779, \"lat\": 50.9583858, \"name\": \"Revelstoke\"},  # ikon\n",
    "            {\"lon\": -117.8194705, \"lat\": 49.1024147, \"name\": \"RED\"},  # ikon\n",
    "            {\"lon\": -117.036177, \"lat\": 34.2248821, \"name\": \"Snow Valley\"},  # ikon\n",
    "            {\"lon\": -116.8892717, \"lat\": 34.2364081, \"name\": \"Snow Summit\"},  # ikon\n",
    "            {\"lon\": -116.8608572, \"lat\": 34.2276766, \"name\": \"Bear Mtn\"},  # ikon\n",
    "            {\"lon\": -116.6227441, \"lat\": 48.3679757, \"name\": \"Schweitzer\"},  # ikon\n",
    "            {\"lon\": -116.2380671, \"lat\": 50.4602801, \"name\": \"Panorama\"},  # ikon\n",
    "            {\"lon\": -116.1621717, \"lat\": 51.4419206, \"name\": \"Lake Louise\"},  # ikon\n",
    "            {\"lon\": -115.7840699, \"lat\": 51.0780997, \"name\": \"Banff\"},  # ikon\n",
    "            {\"lon\": -115.5982699, \"lat\": 51.2037624, \"name\": \"Norquay\"},  # ikon\n",
    "            {\"lon\": -115.5707632, \"lat\": 51.1751675, \"name\": \"SkiBig3\"},  # ikon\n",
    "            {\"lon\": -114.3542874, \"lat\": 43.6949128, \"name\": \"Sun Valley\"},  # ikon\n",
    "            {\"lon\": -114.3461537, \"lat\": 43.6820566, \"name\": \"Dollar Mtn\"},  # ikon\n",
    "            {\"lon\": -111.8571529, \"lat\": 41.2161404, \"name\": \"Snowbasin\"},  # ikon\n",
    "            # The four Cottonwood Canyon resorts all perform similarly.  Combine:\n",
    "            # {\"lon\": -111.6563885, \"lat\": 40.5810814, \"name\": \"Snowbird\"},\n",
    "            {\"lon\": -111.6385807, \"lat\": 40.5884218, \"name\": \"Alta\"},  # ikon\n",
    "            # {\"lon\": -111.591885, \"lat\": 40.619852, \"name\": \"Solitude\"},\n",
    "            # {\"lon\": -111.583187, \"lat\": 40.598019, \"name\": \"Brighton\"},\n",
    "            {\"lon\": -111.5079947, \"lat\": 40.6514199, \"name\": \"Park City\"},  # epic\n",
    "            {\"lon\": -111.478306, \"lat\": 40.63738, \"name\": \"Deer Valley\"},  # ikon\n",
    "            {\"lon\": -111.4012076, \"lat\": 45.2857289, \"name\": \"Big Sky\"},  # ikon\n",
    "            {\"lon\": -110.8279183, \"lat\": 43.5875453, \"name\": \"Jackson Hole\"},  # ikon\n",
    "            {\"lon\": -106.9878231, \"lat\": 38.8697146, \"name\": \"Crested Butte\"},  # ikon\n",
    "            {\"lon\": -105.4545, \"lat\": 36.5959999, \"name\": \"Taos\"},  # ikon\n",
    "        ]\n",
    "    ),\n",
    "    \"na_east\": pd.DataFrame(\n",
    "        [\n",
    "            {\"lon\": -94.9707416, \"lat\": 39.4673048, \"name\": \"Snow Creek\"},  # epic- Kansas City\n",
    "            {\"lon\": -92.7878062, \"lat\": 44.8576608, \"name\": \"Afton\"},  # epic - Minneapolis\n",
    "            {\"lon\": -90.6506898, \"lat\": 38.5353168, \"name\": \"Hidden Valley\"},  # epic\n",
    "            {\"lon\": -88.1876602, \"lat\": 42.4989548, \"name\": \"Wilmot\"},  # epic\n",
    "            {\"lon\": -86.5122305, \"lat\": 38.5555868, \"name\": \"Paoli\"},  # epic\n",
    "            {\"lon\": -84.930067, \"lat\": 45.162884, \"name\": \"Boyne\"},  # ikon\n",
    "            # {\"lon\": -84.926535, \"lat\": 45.4647239, \"name\": \"Boyne Highlands\"},  # ikon\n",
    "            {\"lon\": -83.8115217, \"lat\": 42.54083, \"name\": \"Mt. Brighton\"},  # epic\n",
    "            {\"lon\": -83.6777778, \"lat\": 40.3180556, \"name\": \"Mad River\"},  # epic\n",
    "            {\"lon\": -81.5632108, \"lat\": 41.2640987, \"name\": \"Boston Mills\"},  # epic\n",
    "            {\"lon\": -81.259745, \"lat\": 41.52687, \"name\": \"Alpine Valley\"},  # epic\n",
    "            {\"lon\": -80.3122216, \"lat\": 44.5037818, \"name\": \"Blue Mtn\"},  # ikon\n",
    "            {\"lon\": -79.9960444, \"lat\": 38.4118566, \"name\": \"Snowshoe\"},  # ikon\n",
    "            {\"lon\": -79.2977032, \"lat\": 40.0229768, \"name\": \"7 Springs\"},  # epic\n",
    "            {\"lon\": -79.2581204, \"lat\": 40.058031, \"name\": \"Hidden Valley 2\"},  # epic\n",
    "            {\"lon\": -79.1657908, \"lat\": 40.1638728, \"name\": \"Laurel\"},  # epic\n",
    "            {\"lon\": -77.9333126, \"lat\": 39.7417652, \"name\": \"Whitetail\"},  # epic\n",
    "            {\"lon\": -77.375459, \"lat\": 39.76366, \"name\": \"Liberty\"},  # epic\n",
    "            {\"lon\": -76.9275492, \"lat\": 40.1094506, \"name\": \"Roundtop\"},  # epic\n",
    "            {\"lon\": -75.6563315, \"lat\": 41.1091686, \"name\": \"Jack Frost\"},  # epic\n",
    "            {\"lon\": -75.601282, \"lat\": 41.050189, \"name\": \"Big Boulder\"},  # epic\n",
    "            {\"lon\": -74.5852526, \"lat\": 46.2096417, \"name\": \"Tremblant\"},  # ikon\n",
    "            {\"lon\": -74.2567116, \"lat\": 42.2937298, \"name\": \"Windham\"},  # ikon\n",
    "            {\"lon\": -74.2246402, \"lat\": 42.2028811, \"name\": \"Hunter\"},  # epic\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Assign a color to each region for later plotting purposes, using\n",
    "# Tol's colorblind-friendly \"vibrant\" palette.\n",
    "# https://cran.r-project.org/web/packages/khroma/vignettes/tol.html\n",
    "colors = {\n",
    "    \"japan\": \"#004488\",  # blue\n",
    "    \"alps\": \"#33BBEE\",  # cyan\n",
    "    \"pnw\": \"#009988\",  # teal\n",
    "    \"rockies\": \"#CC3311\",  # red\n",
    "    \"new_england\": \"#DDAA33\",  # yellow\n",
    "    \"europe\": \"#555555\",  # dark grey\n",
    "    \"na_west\": \"#666666\",  # grey\n",
    "    \"na_east\": \"#777777\",  # light grey\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location file: ['ski_resorts_japan.csv', 'ski_resorts_alps.csv', 'ski_resorts_pnw.csv', 'ski_resorts_rockies.csv', 'ski_resorts_new_england.csv', 'ski_resorts_europe.csv', 'ski_resorts_na_west.csv', 'ski_resorts_na_east.csv']\n"
     ]
    }
   ],
   "source": [
    "geo_files = {\n",
    "    region: sk.upload_location_file(\n",
    "        lats=geo[\"lat\"],\n",
    "        lons=geo[\"lon\"],\n",
    "        names=geo[\"name\"],\n",
    "        geoname=f\"ski_resorts_{region}\",\n",
    "        force=force,\n",
    "    )\n",
    "    for region, geo in resorts.items()\n",
    "}\n",
    "\n",
    "# We will later use a Location object to query the Salient API\n",
    "# The functions are capable of handling multiple location files,\n",
    "# so we can pass a vector here.\n",
    "ski_locs = sk.Location(location_file=list(geo_files.values()))\n",
    "print(ski_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire the data\n",
    "\n",
    "For each of the ski resorts, we will get the daily forecast of temperature and precipitation as of the beginning of the season. Then we will also get the historical observed conditions, calculate snowfall, and merge them into a single dataset for later analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Downscale Forecast\n",
    "\n",
    "In contrast to the probabilistic `forecast_timeseries` function, `downscale` samples historical analogs from the forecast distribution to create ensemble timeseries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 15MB\n",
      "Dimensions:        (time: 199, ensemble: 51, location: 92)\n",
      "Coordinates:\n",
      "  * time           (time) datetime64[ns] 2kB 2024-10-15 ... 2025-05-01\n",
      "  * location       (location) object 736B 'Hakuba' 'Rusutsu' ... 'Hunter'\n",
      "    lat            (location) float64 736B 36.69 42.82 42.82 ... 42.29 42.2\n",
      "    lon            (location) float64 736B 137.9 140.7 140.7 ... -74.26 -74.22\n",
      "    forecast_date  datetime64[ns] 8B 2024-10-15\n",
      "    season         (time) object 2kB 2024 2024 2024 2024 ... 2024 2024 2024 <NA>\n",
      "    season_day     (time) object 2kB 14 15 16 17 18 19 ... 208 209 210 211 <NA>\n",
      "Dimensions without coordinates: ensemble\n",
      "Data variables:\n",
      "    temp_anom      (ensemble, time, location) float32 4MB 5.524 5.205 ... -1.876\n",
      "    precip_anom    (ensemble, time, location) float32 4MB -7.229 ... -0.994\n",
      "    temp           (ensemble, time, location) float32 4MB 16.77 15.2 ... 9.413\n",
      "    precip         (ensemble, time, location) float32 4MB 0.0 23.1 ... 2.597\n"
     ]
    }
   ],
   "source": [
    "fcst_files = sk.downscale(\n",
    "    loc=ski_locs,\n",
    "    variables=vars,\n",
    "    members=51,\n",
    "    date=fcst_date,\n",
    "    force=force,\n",
    ")\n",
    "\n",
    "# Because we are requesting multiple location_files, fcst_files is a\n",
    "# table with multiple downscale files.  Let's combine all of them:\n",
    "fcst = xr.open_mfdataset(\n",
    "    fcst_files[\"file_name\"].values,\n",
    "    concat_dim=\"location\",\n",
    "    combine=\"nested\",\n",
    ")\n",
    "# Align the data to the ski season:\n",
    "fcst = fcst.sel(forecast_day=slice(fcst_date, fcst_end))\n",
    "\n",
    "# We use scientific units for precip like mm day-1\n",
    "# Let's make this more readable for plotting purposes:\n",
    "fcst[\"precip\"].attrs[\"units\"] = \"mm/day\"\n",
    "\n",
    "# rename \"forecast_day\" to \"time\" to match the output from data_timeseries\n",
    "fcst = fcst.rename({\"forecast_day\": \"time\"})\n",
    "\n",
    "# Remove things we don't need:\n",
    "fcst = fcst.drop_vars([\"temp_clim\", \"precip_clim\", \"analog\"])\n",
    "\n",
    "\n",
    "def add_season(ds, season_start_month=10, season_end_month=5):\n",
    "    \"\"\"Add season and season_day coordinates to a dataset based on time.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset with a 'time' coordinate\n",
    "        season_start_month: Month when season starts (e.g. 10 for October)\n",
    "        season_end_month: Month when season ends (e.g. 5 for May)\n",
    "\n",
    "    Returns:\n",
    "        xarray Dataset with new 'season' and 'season_day' coordinates\n",
    "    \"\"\"\n",
    "    time = ds.time\n",
    "    month = time.dt.month.values\n",
    "    year = time.dt.year.values\n",
    "\n",
    "    # Calculate season_day relative to each year's season start\n",
    "    season_starts = pd.to_datetime([f\"{y}-{season_start_month:02d}-01\" for y in year])\n",
    "    time_np = time.values.astype(\"datetime64[D]\")\n",
    "    season_starts_np = season_starts.values.astype(\"datetime64[D]\")\n",
    "\n",
    "    # For dates before October, use previous year's October 1\n",
    "    # Account for leap years in the offset\n",
    "    is_leap = pd.to_datetime(season_starts_np).is_leap_year\n",
    "    year_days = np.where(is_leap, 366, 365)\n",
    "\n",
    "    days = (\n",
    "        (\n",
    "            time_np\n",
    "            - np.where(\n",
    "                month < season_start_month,\n",
    "                season_starts_np - np.timedelta64(1, \"D\") * year_days,\n",
    "                season_starts_np,\n",
    "            )\n",
    "        )\n",
    "        .astype(\"timedelta64[D]\")\n",
    "        .astype(int)\n",
    "    )\n",
    "    is_summer = (month >= season_end_month) & (month < season_start_month)\n",
    "    season_day = np.where(is_summer, pd.NA, days)\n",
    "\n",
    "    # Calculate season year\n",
    "    season = year\n",
    "    season = np.where(month < season_start_month, season - 1, season)\n",
    "    season = np.where(is_summer, pd.NA, season)\n",
    "\n",
    "    # Add new coordinates\n",
    "    ds = ds.assign_coords({\"season\": (\"time\", season), \"season_day\": (\"time\", season_day)})\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "fcst = add_season(fcst)\n",
    "\n",
    "\n",
    "fcst = fcst.compute()\n",
    "\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Historical Data\n",
    "\n",
    "The `data_timeseries` function will load the historical daily ERA5 timeseries, which we can later compare to the `downscale` timeseries ensembles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 19MB\n",
      "Dimensions:        (time: 12451, location: 92)\n",
      "Coordinates:\n",
      "  * time           (time) datetime64[ns] 100kB 1990-10-01 ... 2024-11-01\n",
      "  * location       (location) object 736B '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "    lat            (location) float64 736B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon            (location) float64 736B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file  (location) object 736B 'ski_resorts_na_east.csv' ... 'ski_...\n",
      "    season         (time) object 100kB 1990 1990 1990 1990 ... 2024 2024 2024\n",
      "    season_day     (time) object 100kB 0 1 2 3 4 5 6 7 ... 25 26 27 28 29 30 31\n",
      "    region         (location) <U11 4kB 'na_east' 'rockies' ... 'rockies' 'alps'\n",
      "    color          (location) <U7 3kB '#777777' '#CC3311' ... '#33BBEE'\n",
      "Data variables:\n",
      "    temp           (time, location) float64 9MB 10.52 6.065 ... -5.47 4.046\n",
      "    precip         (time, location) float64 9MB 0.01205 0.2661 2.719 ... 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get historical observed performance for each ski resort\n",
    "hist_files = sk.data_timeseries(\n",
    "    loc=ski_locs,\n",
    "    variable=vars,\n",
    "    field=\"vals\",\n",
    "    start=hist_start,\n",
    "    end=min(fcst_end, today),\n",
    "    frequency=\"daily\",\n",
    "    force=force,\n",
    ")\n",
    "\n",
    "# Assemble each historical file into a single xarray dataset\n",
    "hist = sk.load_multihistory(hist_files)\n",
    "hist = add_season(hist)\n",
    "\n",
    "# Assign a color to each region for plotting purposes later\n",
    "prefix = \"ski_resorts_\"\n",
    "suffix = \".csv\"\n",
    "region = [x.replace(prefix, \"\").replace(suffix, \"\") for x in hist[\"location_file\"].values]\n",
    "regcol = [colors[reg] for reg in region]\n",
    "hist = hist.assign_coords(region=(\"location\", region), color=(\"location\", regcol))\n",
    "\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add history before forecast starts\n",
    "\n",
    "We begin analyzing each ski season in October to account for snow accumulation before the mountains become skiable. If the forecast was generated after October, prepend the observed history to the forecast timeseries so that we account for weather before the forecast was generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 24MB\n",
      "Dimensions:        (location: 92, ensemble: 51, time: 213)\n",
      "Coordinates:\n",
      "  * location       (location) object 736B '7 Springs' 'A-Basin' ... 'Zermatt'\n",
      "  * ensemble       (ensemble) int64 408B 0 1 2 3 4 5 6 ... 44 45 46 47 48 49 50\n",
      "  * time           (time) datetime64[ns] 2kB 2024-10-01 ... 2025-05-01\n",
      "    lat            (location) float64 736B 40.02 39.64 44.86 ... 39.89 46.02\n",
      "    lon            (location) float64 736B -79.3 -105.9 -92.79 ... -105.8 7.752\n",
      "    location_file  (location) object 736B 'ski_resorts_na_east.csv' ... 'ski_...\n",
      "    season         (time) object 2kB 2024 2024 2024 2024 ... 2024 2024 2024 <NA>\n",
      "    season_day     (time) object 2kB 0 1 2 3 4 5 6 ... 207 208 209 210 211 <NA>\n",
      "    region         (location) <U11 4kB 'na_east' 'rockies' ... 'rockies' 'alps'\n",
      "    color          (location) <U7 3kB '#777777' '#CC3311' ... '#33BBEE'\n",
      "    forecast_date  datetime64[ns] 8B 2024-10-15\n",
      "Data variables:\n",
      "    temp           (ensemble, time, location) float64 8MB 15.71 7.041 ... 1.195\n",
      "    precip         (ensemble, time, location) float64 8MB 22.07 ... 37.29\n",
      "    temp_anom      (ensemble, time, location) float32 4MB nan nan ... -0.4476\n",
      "    precip_anom    (ensemble, time, location) float32 4MB nan nan ... 31.37\n"
     ]
    }
   ],
   "source": [
    "season_start = f\"{year}-{start_month}-01\"\n",
    "if fcst.time[0] > np.datetime64(season_start):\n",
    "    fcst = xr.concat(\n",
    "        [\n",
    "            hist.sel(time=slice(season_start, fcst.time[0] - pd.Timedelta(days=1)))\n",
    "            .expand_dims(ensemble=fcst.ensemble)\n",
    "            .transpose(\"ensemble\", \"time\", \"location\"),\n",
    "            fcst,\n",
    "        ],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Snow Water Equivalent\n",
    "\n",
    "The `calc_swe` function builds on the `snow17` model to calculate the snow water equivalent (SWE) at each location and for each ensemble. It requires that the dataset input has data values `precip` and `temp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp         (ensemble, time, location) float64 8MB 15.71 7.041 ... 1.195\n",
      "    precip       (ensemble, time, location) float64 8MB 22.07 0.001423 ... 37.29\n",
      "    temp_anom    (ensemble, time, location) float32 4MB nan nan ... -0.4476\n",
      "    precip_anom  (ensemble, time, location) float32 4MB nan nan ... -1.651 31.37\n",
      "    swe          (ensemble, time, location) float64 8MB 0.0 0.0 ... 272.6 576.1\n",
      "    swe_avg      (location) float64 736B 5.799 125.2 19.76 ... 25.7 113.0 303.2\n"
     ]
    }
   ],
   "source": [
    "if \"swe\" not in fcst:\n",
    "    fcst[\"swe\"] = sk.hydro.calc_swe(fcst, \"time\")\n",
    "\n",
    "fcst[\"swe_avg\"] = fcst[\"swe\"].mean([\"ensemble\", \"time\"])\n",
    "\n",
    "print(fcst.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp     (time, location) float64 9MB 10.52 6.065 13.48 ... -5.47 4.046\n",
      "    precip   (time, location) float64 9MB 0.01205 0.2661 2.719 ... 0.0 0.0\n",
      "    swe      (time, location) float64 9MB 0.0 0.0 0.0 0.0 ... 0.0 0.0 6.12 0.0\n"
     ]
    }
   ],
   "source": [
    "if \"swe\" not in hist:\n",
    "    hist[\"swe\"] = sk.hydro.calc_swe(hist, \"time\")\n",
    "\n",
    "print(hist.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a seasonal average similar to the forecast's per-ensemble average we need to break the single linear `time` dimension into `season` + `season_day` dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data variables:\n",
      "    temp     (location, season, season_day) float64 5MB 10.52 12.35 ... nan nan\n",
      "    precip   (location, season, season_day) float64 5MB 0.01205 0.03876 ... nan\n",
      "    swe      (location, season, season_day) float64 5MB 0.0 0.0 0.0 ... nan nan\n",
      "    time     (season, season_day) datetime64[ns] 60kB 1990-10-01 ... NaT\n",
      "    swe_avg  (location) float64 736B 16.78 135.3 33.22 ... 40.65 116.2 303.8\n"
     ]
    }
   ],
   "source": [
    "def stack_by_season(ds):\n",
    "    \"\"\"Convert a dataset from time dimension to season/season_day dimensions.\n",
    "\n",
    "    Args:\n",
    "        ds: xarray Dataset with time dimension and season/season_day coordinates\n",
    "\n",
    "    Returns:\n",
    "        xarray Dataset with season and season_day dimensions instead of time\n",
    "    \"\"\"\n",
    "    valid_mask = ~ds[\"season\"].isnull() & ~ds[\"season_day\"].isnull()\n",
    "    ds_clean = ds.isel(time=valid_mask).copy()\n",
    "    ds_clean[\"season\"] = ds_clean.season.astype(int)\n",
    "    ds_clean[\"season_day\"] = ds_clean.season_day.astype(int)\n",
    "    time_df = pd.DataFrame(\n",
    "        {\n",
    "            \"season\": ds_clean.season.values,\n",
    "            \"season_day\": ds_clean.season_day.values,\n",
    "            \"time\": ds_clean.time.values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds_stacked = ds_clean.set_index(time=[\"season\", \"season_day\"])\n",
    "    ds_final = ds_stacked.unstack(\"time\")\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [ds_final.season.values, ds_final.season_day.values], names=[\"season\", \"season_day\"]\n",
    "    )\n",
    "    time_series = time_df.set_index([\"season\", \"season_day\"])[\"time\"].reindex(idx)\n",
    "    ds_final[\"time\"] = xr.DataArray(\n",
    "        time_series.values.reshape(len(ds_final.season), len(ds_final.season_day)),\n",
    "        dims=[\"season\", \"season_day\"],\n",
    "    )\n",
    "\n",
    "    return ds_final\n",
    "\n",
    "\n",
    "hist = stack_by_season(hist)\n",
    "hist[\"swe_avg\"] = hist[\"swe\"].mean([\"season\", \"season_day\"])\n",
    "print(hist.data_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can later merge, let's similarly denominate the forecast by `season_day` instead of `time`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = stack_by_season(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge\n",
    "\n",
    "Combine the historical and forecast datasets into a single dataset so that we can make sure they are aligned by `location`.\n",
    "\n",
    "We don't want to highlight resorts with below-average snowfall, so let's sort the dataset and cut out the bottom half.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 568MB\n",
      "Dimensions:           (season: 35, season_day: 213, location: 46, ensemble: 51)\n",
      "Coordinates:\n",
      "  * season            (season) int64 280B 1990 1991 1992 1993 ... 2022 2023 2024\n",
      "  * season_day        (season_day) int64 2kB 0 1 2 3 4 5 ... 208 209 210 211 212\n",
      "  * location          (location) object 368B 'Park City' ... 'Whistler'\n",
      "  * ensemble          (ensemble) int64 408B 0 1 2 3 4 5 6 ... 45 46 47 48 49 50\n",
      "    lat               (location) float64 368B 40.65 50.88 45.29 ... 47.74 50.11\n",
      "    lon               (location) float64 368B -111.5 -119.9 ... -121.1 -122.9\n",
      "    location_file     (location) object 368B 'ski_resorts_na_west.csv' ... 's...\n",
      "    region            (location) <U11 2kB 'na_west' 'na_west' ... 'pnw' 'pnw'\n",
      "    color             (location) <U7 1kB '#666666' '#666666' ... '#009988'\n",
      "    forecast_date     datetime64[ns] 8B 2024-10-15\n",
      "Data variables:\n",
      "    temp_fcst         (ensemble, location, season, season_day) float64 140MB ...\n",
      "    precip_fcst       (ensemble, location, season, season_day) float64 140MB ...\n",
      "    temp_anom_fcst    (ensemble, location, season, season_day) float32 70MB n...\n",
      "    precip_anom_fcst  (ensemble, location, season, season_day) float32 70MB n...\n",
      "    swe_fcst          (ensemble, location, season, season_day) float64 140MB ...\n",
      "    swe_avg_fcst      (location) float64 368B 65.26 72.56 87.43 ... 364.2 597.8\n",
      "    time_fcst         (season, season_day) datetime64[ns] 60kB NaT NaT ... NaT\n",
      "    temp_hist         (location, season, season_day) float64 3MB 12.15 ... nan\n",
      "    precip_hist       (location, season, season_day) float64 3MB 0.03106 ... nan\n",
      "    swe_hist          (location, season, season_day) float64 3MB 0.0 0.0 ... nan\n",
      "    time_hist         (season, season_day) datetime64[ns] 60kB 1990-10-01 ......\n",
      "    swe_avg_hist      (location) float64 368B 116.1 90.66 113.2 ... 437.1 635.2\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "none shall pass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m met \u001b[38;5;241m=\u001b[39m met\u001b[38;5;241m.\u001b[39misel(location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mlen\u001b[39m(met\u001b[38;5;241m.\u001b[39mlocation) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(met)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone shall pass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: none shall pass"
     ]
    }
   ],
   "source": [
    "met = xr.merge(\n",
    "    [\n",
    "        fcst.rename({var: f\"{var}_fcst\" for var in fcst.data_vars}),\n",
    "        hist.rename({var: f\"{var}_hist\" for var in hist.data_vars}),\n",
    "    ]\n",
    ")\n",
    "# We're interested in the resorts that have better-than-average snowfall:\n",
    "met = met.sortby(\"swe_avg_fcst\")\n",
    "# The lowest-snowfall resorts are dominated by snowmaking.  Remove them:\n",
    "met = met.isel(location=slice(len(met.location) // 2, None))\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(\n",
    "    fcst: xr.DataArray,\n",
    "    hist: xr.DataArray = None,\n",
    "    title: str = \"\",\n",
    "    legend_loc: str = \"center right\",\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"Plot predictions and observed values as a box-and-whisker plot.\"\"\"\n",
    "    # extract a table of seasonal averages per location\n",
    "    # if the length of the time dimension is >1, take the mean.  Otherwise, we're good.\n",
    "    # if \"time\" in fcst.dims:\n",
    "    xlab = \"Season Mean\"\n",
    "    avg = fcst.sel(season=year).mean(dim=\"season_day\")\n",
    "    # else:\n",
    "    #    xlab = str(np.datetime_as_string(fcst.time.values, unit=\"D\"))\n",
    "    #    avg = fcst\n",
    "    avg = avg.to_dataframe(dim_order=[\"location\", \"ensemble\"])\n",
    "    avg = avg[fcst.name].unstack(level=0).to_numpy()\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 10))  # Create a new figure if no axis is provided\n",
    "\n",
    "    plt_box = ax.boxplot(\n",
    "        avg,\n",
    "        showfliers=False,\n",
    "        vert=False,\n",
    "        labels=fcst.location.values,\n",
    "        patch_artist=True,\n",
    "        meanline=True,\n",
    "        showmeans=True,\n",
    "        meanprops=dict(linewidth=1, color=\"white\"),\n",
    "        medianprops=dict(linewidth=0, color=\"gray\", alpha=0),\n",
    "        whiskerprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "        capprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "        boxprops=dict(linewidth=0.7, color=\"gray\"),\n",
    "    )\n",
    "    [patch.set_facecolor(color) for patch, color in zip(plt_box[\"boxes\"], fcst.color.values)]\n",
    "    ax.set_xlabel(f\"{xlab} {fcst.long_name} ({fcst.units})\")\n",
    "    ax.set_title(title)\n",
    "    legend_names = [\"japan\", \"alps\", \"pnw\", \"rockies\", \"new_england\"]\n",
    "    legend_handles = plt_box[\"boxes\"][: len(legend_names)]\n",
    "\n",
    "    if isinstance(hist, xr.DataArray):\n",
    "        if \"time\" in hist.dims and hist.dims[\"time\"] > 1:\n",
    "            obs = hist.mean(dim=\"time\")\n",
    "        else:\n",
    "            obs = hist\n",
    "\n",
    "        plt_hist = ax.plot(obs, np.arange(len(hist.location)) + 1, color=\"black\", linewidth=2)\n",
    "        legend_handles += [plt_hist[0]]\n",
    "        legend_names += [\"Observed\"]\n",
    "    elif isinstance(hist, float):\n",
    "        # plot a grey dotted vertical line at the value of hist\n",
    "        plt_hist = ax.axvline(hist, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    else:\n",
    "        plt_hist = None\n",
    "\n",
    "    if legend_loc != \"none\":\n",
    "        leg = ax.legend(legend_handles, legend_names, loc=legend_loc)\n",
    "        for patch, reg in zip(leg.get_patches(), legend_names):\n",
    "            patch.set_facecolor(colors[reg])\n",
    "\n",
    "\n",
    "plot_boxes(met[\"swe_fcst\"], title=\"Snow Forecast by Location\")\n",
    "\n",
    "# plot_boxes(met[\"swe_fcst\"], met[\"swe_hist\"], \"Snow Forecast by Location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Drivers\n",
    "\n",
    "Investigate the differences between forecast and actual temp/precip to see which contributed more to snow forecast error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_diff = met[\"temp_fcst\"] - met[\"temp_hist\"]\n",
    "temp_diff.name = \"temp_diff\"\n",
    "temp_diff.attrs[\"long_name\"] = \"Temperature Error\"\n",
    "temp_diff.attrs[\"units\"] = \"C\"\n",
    "\n",
    "precip_diff = met[\"precip_fcst\"] - met[\"precip_hist\"]\n",
    "precip_diff.name = \"precip_diff\"\n",
    "precip_diff.attrs[\"long_name\"] = \"Precipitation Error\"\n",
    "precip_diff.attrs[\"units\"] = \"mm\"\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10))\n",
    "plot_boxes(temp_diff, 0.0, title=\"Temp Error by Location\", legend_loc=\"center left\", ax=ax1)\n",
    "plot_boxes(precip_diff, 0.0, title=\"Precip Error by Location\", legend_loc=\"none\", ax=ax2)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Variation at 4 Resorts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_names = [\"Whistler\", \"Andermatt-Sedrun\", \"Copper\", \"Sugarloaf\"]\n",
    "\n",
    "\n",
    "def plot_ensembles(loc, var=\"temp\", title=False):\n",
    "    \"\"\"Show ensemble values for a given variable.\"\"\"\n",
    "    x_val = \"season_day\"\n",
    "    var_fcst = f\"{var}_fcst\"\n",
    "    # var_hist = f\"{var}_hist\"\n",
    "\n",
    "    loc_plt = loc[var_fcst].plot.line(x=x_val, color=\"grey\", alpha=0.1, add_legend=False)\n",
    "    # obs_plt = loc[var_hist].plot.line(\n",
    "    #    x=x_val, color=\"black\", linestyle=\"-\", linewidth=2, add_legend=False\n",
    "    # )\n",
    "    avg_plt = (\n",
    "        loc[var_fcst]\n",
    "        .mean(dim=\"ensemble\", keep_attrs=True)\n",
    "        .plot.line(x=x_val, color=loc[\"color\"].values.tolist(), linewidth=2, add_legend=False)\n",
    "    )\n",
    "\n",
    "    plt.title(loc[\"location\"].values if title else \"\")\n",
    "    plt.xlabel(\"\")\n",
    "\n",
    "\n",
    "(fig, axs) = plt.subplots(\n",
    "    nrows=3,\n",
    "    ncols=len(focus_names),\n",
    "    sharex=True,\n",
    "    sharey=\"row\",\n",
    "    figsize=(5 * len(focus_names), 15),\n",
    ")\n",
    "\n",
    "for idx in range(len(focus_names)):\n",
    "    loc = met.sel(location=focus_names[idx], season=year)\n",
    "    plt.sca(axs[0, idx])\n",
    "    plot_ensembles(loc, \"swe\", title=True)\n",
    "    plt.sca(axs[1, idx])\n",
    "    plot_ensembles(loc, \"temp\")\n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "    plt.sca(axs[2, idx])\n",
    "    plot_ensembles(loc, \"precip\")\n",
    "    plt.gca().set_ylim((0, 40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('salient')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "51f8f0b1da789c29b6dfdfd0d04fb138f3b7f54f0cf7fdaf39807db9fc45e326"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
