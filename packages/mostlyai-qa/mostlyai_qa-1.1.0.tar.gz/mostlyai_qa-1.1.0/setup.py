# -*- coding: utf-8 -*-
from setuptools import setup

package_dir = \
{'': 'src'}

packages = \
['mostlyai', 'mostlyai.qa', 'mostlyai.qa.assets']

package_data = \
{'': ['*'],
 'mostlyai.qa.assets': ['embedders/sentence-transformers/all-MiniLM-L6-v2/*',
                        'embedders/sentence-transformers/all-MiniLM-L6-v2/1_Pooling/*',
                        'html/*',
                        'tokenizers/transformers/gpt2/*']}

install_requires = \
['Jinja2>=3.1.2',
 'fastcluster>=1.2.6',
 'joblib>=1.2.0',
 'numpy>=1.26.3,<2.0.0',
 'pandas>=2.2.0',
 'phik>=0.12.4',
 'plotly>=5.18.0',
 'pyarrow>=16.0.0',
 'scikit-learn>=1.4.0',
 'scipy>=1.11.0',
 'sentence-transformers>=3.1.0']

setup_kwargs = {
    'name': 'mostlyai-qa',
    'version': '1.1.0',
    'description': 'Quality assurance for synthetic data',
    'long_description': '# Synthetic Data - Quality Assurance\n\nAssess the fidelity and novelty of synthetic samples with respect to original samples:\n\n1. calculate a rich set of accuracy, similarity and distance metrics\n2. visualize statistics for easy comparison to training and holdout samples\n3. generate a standalone, easy-to-share, easy-to-read HTML summary report\n\n...all with a single line of Python code ðŸ’¥.\n\n## Getting Started\n\n### Installation\n\n```bash\npip install -U mostlyai-qa\n```\n\n### Basic Usage\n\n```python\nfrom mostlyai import qa\n\n# analyze single-table data\nreport_path, metrics = qa.report(\n    syn_tgt_data = synthetic_df,\n    trn_tgt_data = training_df,\n    hol_tgt_data = holdout_df,  # optional\n)\n\n# analyze sequential data\nreport_path, metrics = qa.report(\n    syn_tgt_data = synthetic_df,\n    trn_tgt_data = training_df,\n    hol_tgt_data = holdout_df,  # optional\n    tgt_context_key = "user_id",\n)\n\n# analyze sequential data with context\nreport_path, metrics = qa.report(\n    syn_tgt_data = synthetic_df,\n    trn_tgt_data = training_df,\n    hol_tgt_data = holdout_df,  # optional\n    syn_ctx_data = synthetic_context_df,\n    trn_ctx_data = training_context_df,\n    hol_ctx_data = holdout_context_df,  # optional\n    ctx_primary_key = "id",\n    tgt_context_key = "user_id",\n)\n```\n\n### Syntax\n\n```python\ndef report(\n    *,\n    syn_tgt_data: pd.DataFrame,\n    trn_tgt_data: pd.DataFrame,\n    hol_tgt_data: pd.DataFrame | None = None,\n    syn_ctx_data: pd.DataFrame | None = None,\n    trn_ctx_data: pd.DataFrame | None = None,\n    hol_ctx_data: pd.DataFrame | None = None,\n    ctx_primary_key: str | None = None,\n    tgt_context_key: str | None = None,\n    report_path: str | Path | None = "model-report.html",\n    report_title: str = "Model Report",\n    report_subtitle: str = "",\n    report_credits: str = REPORT_CREDITS,\n    report_extra_info: str = "",\n    max_sample_size_accuracy: int | None = None,\n    max_sample_size_embeddings: int | None = None,\n    statistics_path: str | Path | None = None,\n    on_progress: ProgressCallback | None = None,\n) -> tuple[Path, dict | None]:\n    """\n    Generate HTML report and metrics for comparing synthetic and original data samples.\n\n    Args:\n        syn_tgt_data: Synthetic samples\n        trn_tgt_data: Training samples\n        hol_tgt_data: Holdout samples\n        syn_ctx_data: Synthetic context samples\n        trn_ctx_data: Training context samples\n        hol_ctx_data: Holdout context samples\n        ctx_primary_key: Column within the context data that contains the primary key\n        tgt_context_key: Column within the target data that contains the key to link to the context\n        report_path: Path of where to store the HTML report\n        report_title: Title of the HTML report\n        report_subtitle: Subtitle of the HTML report\n        report_credits: Credits of the HTML report\n        report_extra_info: Extra details to be included to the HTML report\n        max_sample_size_accuracy: Max sample size for accuracy\n        max_sample_size_embeddings: Max sample size for embeddings (similarity & distances)\n        statistics_path: Path of where to store the statistics to be used by `report_from_statistics`\n        on_progress: A custom progress callback\n    Returns:\n        1. Path to the HTML report\n        2. Dictionary of calculated metrics:\n        - `accuracy`:  # Accuracy is defined as (100% - Total Variation Distance), for each distribution, and then averaged across.\n          - `overall`: Overall accuracy of synthetic data, i.e. average across univariate, bivariate and coherence.\n          - `univariate`: Average accuracy of discretized univariate distributions.\n          - `bivariate`: Average accuracy of discretized bivariate distributions.\n          - `coherence`: Average accuracy of discretized coherence distributions. Only applicable for sequential data.\n          - `overall_max`: Expected overall accuracy of a same-sized holdout. Serves as reference for `overall`.\n          - `univariate_max`: Expected univariate accuracy of a same-sized holdout. Serves as reference for `univariate`.\n          - `bivariate_max`: Expected bivariate accuracy of a same-sized holdout. Serves as reference for `bivariate`.\n          - `coherence_max`: Expected coherence accuracy of a same-sized holdout. Serves as reference for `coherence`.\n        - `similarity`:  # All similarity metrics are calculated within an embedding space.\n            - `cosine_similarity_training_synthetic`: Cosine similarity between training and synthetic centroids.\n            - `cosine_similarity_training_holdout`: Cosine similarity between training and holdout centroids. Serves as reference for `cosine_similarity_training_synthetic`.\n            - `discriminator_auc_training_synthetic`: Cross-validated AUC of a discriminative model to distinguish between training and synthetic samples.\n            - `discriminator_auc_training_holdout`: Cross-validated AUC of a discriminative model to distinguish between training and holdout samples. Serves as reference for `discriminator_auc_training_synthetic`.\n        - `distances`:  # All distance metrics are calculated within an embedding space. An equal number of training and holdout samples is considered.\n            - `ims_training`: Share of synthetic samples that are identical to a training sample.\n            - `ims_holdout`: Share of synthetic samples that are identical to a holdout sample. Serves as reference for `ims_training`.\n            - `dcr_training`: Average L2 nearest-neighbor distance between synthetic and training samples.\n            - `dcr_holdout`: Average L2 nearest-neighbor distance between synthetic and holdout samples. Serves as reference for `dcr_training`.\n            - `dcr_share`: Share of synthetic samples that are closer to a training sample than to a holdout sample. This shall not be significantly larger than 50\\%.\n    """\n```\n\n## Metrics\n\nWe calculate three sets of metrics to compare synthetic data with the original data.\n\n### Accuracy\n\nWe calculate discretized marginal distributions for all columns, to then calculate the L1 distance between the synthetic and the original training data.\nThe reported accuracy is then expressed as 100% minus the total variational distance (TVD), which is half the L1 distance between the two distributions.\nWe then average across these accuracies to get a single accuracy score. The higher the score, the better the synthetic data.\n\n1. **Univariate Accuracy**: We measure the accuracy for the univariate distributions for all target columns.\n2. **Bivariate Accuracy**: We measure the accuracy for all pair-wise distributions for target columns, as well as for target columns with respect to the context columns.\n3. **Coherence Accuracy**: We measure the accuracy for the auto-correlation for all target columns. Only applicable for sequential data.\n\nAn overall accuracy score is then calculated as the average of these aggregate-level scores.\n\n### Similarity\n\nWe embed all records into an embedding space, to calculate two metrics:\n\n1. **Cosine Similarity**: We calculate the cosine similarity between the centroids of the synthetic and the original training data. This is then compared to the cosine similarity between the centroids of the original training and holdout data. The higher the score, the better the synthetic data.\n2. **Discriminator AUC**: We train a binary classifier to check whether one can distinguish between synthetic and original training data based on their embeddings. This is again compared to the same metric for the original training and holdout data. A score close to 50% indicates, that synthetic samples are indistinguishable from original samples.\n\n### Distances\n\nWe again embed all records into an embedding space, to then measure individual-level L2 distances between samples. For each synthetic sample, we calculate the distance to the nearest original sample (DCR). We once do this with respect to original training records, and once with respect to holdout records, and then compare these DCRs to each other. For privacy-safe synthetic data we expect to see that synthetic data is just as close to original training data, as it is to original holdout data.\n\n## Sample HTML Report\n\n![Metrics](./docs/screenshots/metrics.png)\n![Accuracy Univariates](./docs/screenshots/accuracy_univariates.png)\n![Accuracy Bivariates](./docs/screenshots/accuracy_bivariates.png)\n![Accuracy Coherence](./docs/screenshots/accuracy_coherence.png)\n![Similarity](./docs/screenshots/similarity.png)\n![Distances](./docs/screenshots/distances.png)\n\nSee the [examples](./examples/) directory for further examples.\n',
    'author': 'MOSTLY AI',
    'author_email': 'dev@mostly.ai',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'package_dir': package_dir,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.10,<4.0',
}


setup(**setup_kwargs)
