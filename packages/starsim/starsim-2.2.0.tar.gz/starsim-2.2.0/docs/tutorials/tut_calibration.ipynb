{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T7 - Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "An interactive version of this notebook is available on [Google Colab](https://colab.research.google.com/github/starsimhub/starsim/blob/main/docs/tutorials/tut_calibration.ipynb?install=starsim) or [Binder](https://mybinder.org/v2/gh/starsimhub/starsim/HEAD?labpath=docs%2Ftutorials%2Ftut_calibration.ipynb).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disease models typically require contextualization to a relevant setting of interest prior to addressing \"what-if\" scenario questions. The process of tuning model input parameters so that model outputs match observed data is known as calibration. There are many approaches to model calibration, ranging from manual tuning to fully Bayesian methods.\n",
    "\n",
    "For many applications, we have found that an optimization-based approach is sufficient. Such methods avoid the tedious process of manual tuning and are less computationally expensive than fully Bayesian methods. One such optimization-based approach is the Optuna library, which is a Bayesian hyperparameter optimization framework. Optuna is designed for tuning hyperparameters of machine learning models, but it can also be used to calibrate disease models.\n",
    "\n",
    "Calibration libraries often treat the disease model as a black box, where the input parameters are the \"hyperparameters\" to be tuned. The calibration process is often iterative and requires a combination of expert knowledge and computational tools. The optimization algorithm iteratively chooses new parameter values to evaluate, and the model is run with these values to generate outputs. The outputs are compared to observed data, and a loss function is calculated to quantify the difference between the model outputs and the observed data. The optimization algorithm then uses this loss function to update its search strategy and choose new parameter values to evaluate. This process continues until the algorithm converges to a set of parameter values that minimize the loss function.\n",
    "\n",
    "While many optimization algorithms are available, Starsim has a built-in interface to the Optuna library, which we will demonstrate in this tutorial. We will use a simple Susceptible-Infected-Recovered (SIR) model as an example. We will tune three input parameters, the infectivity parameter, `beta`, the initial prevalence parameter, `init_prev`, and the Poisson-distributed degree distribution parameter, `n_contacts`. We will calibrate the model using a beta-binomial likelihood function so as to match prevalence at three distinct time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a few imports and default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports and settings\n",
    "import sciris as sc\n",
    "import starsim as ss\n",
    "import pandas as pd\n",
    "\n",
    "n_agents = 2e3\n",
    "debug = False # If true, will run in serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration class will require a base `Sim` object. This `sim` will later be modified according to parameters selected by the optimization engine. The following function creates the base `Sim` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sim():\n",
    "    \"\"\" Helper function to create the base simulation object \"\"\"\n",
    "    sir = ss.SIR(\n",
    "        beta = ss.beta(0.075),\n",
    "        init_prev = ss.bernoulli(0.02),\n",
    "    )\n",
    "    random = ss.RandomNet(n_contacts=ss.poisson(4))\n",
    "\n",
    "    sim = ss.Sim(\n",
    "        n_agents = n_agents,\n",
    "        start = sc.date('1990-01-01'),\n",
    "        dur = 40,\n",
    "        dt = 1,\n",
    "        unit = 'day',\n",
    "        diseases = sir,\n",
    "        networks = random,\n",
    "        verbose = 0,\n",
    "    )\n",
    "\n",
    "    # Remember to return the sim object\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the calibration parameters. These are the inputs that Optuna will be able to modify. Here, we define three such parameters, `beta`, `init_prev`, and `n_contacts`.\n",
    "\n",
    "Each parameter entry should have range defined by `low` and `high` as well as a `guess` values. The `guess` value is not used by Optuna, rather only for a check after calibration completes to see if the new parameters are better than the `guess` values.\n",
    "\n",
    "You'll notice there are a few other parameters that can be specified. For example, the data type of the parameter appears in `suggest_type`. Possible values are listed in the Optuna documentation, and include [suggest_float](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float) for float values and [suggest_int](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_int) for integer types.\n",
    "\n",
    "To make things easier for the search algorithm, it's helpful to indicate how outputs are expected to change with inputs. For example, increasing `beta` from 0.01 to 0.02 should double disease transmission, but increasing from 0.11 to 0.12 will have a small effect. Thus, we indicate that this parameter should be calibrated with `log=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calibration parameters\n",
    "calib_pars = dict(\n",
    "    beta = dict(low=0.01, high=0.30, guess=0.15, suggest_type='suggest_float', log=True), # Note the log scale\n",
    "    init_prev = dict(low=0.01, high=0.05, guess=0.15), # Default type is suggest_float, no need to re-specify\n",
    "    n_contacts = dict(low=2, high=10, guess=3, suggest_type='suggest_int'), # Suggest int just for this demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization engine iteratively chooses input parameters to simulate. Those parameters are passed into the following `build_sim` function as a dictionary of `calib_pars` along with the base `sim` and any other key word arguments. The `calib_pars` will be as above, but importantly will have an additional key named `value` containing the value selected by Optuna.\n",
    "\n",
    "When modifying a `sim`, it is important to realize that the simulation has not been initialized yet. Nonetheless, the configuration is available for modification at `sim.pars`, as demonstrated in the function below for the SIR example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sim(sim, calib_pars, **kwargs):\n",
    "    \"\"\" Modify the base simulation by applying calib_pars \"\"\"\n",
    "\n",
    "    sir = sim.pars.diseases # There is only one disease in this simulation and it is a SIR\n",
    "    net = sim.pars.networks # There is only one network in this simulation and it is a RandomNet\n",
    "\n",
    "    for k, pars in calib_pars.items(): # Loop over the calibration parameters\n",
    "        if k == 'rand_seed':\n",
    "            sim.pars.rand_seed = v\n",
    "            continue\n",
    "\n",
    "        # Each item in calib_pars is a dictionary with keys like 'low', 'high',\n",
    "        # 'guess', 'suggest_type', and importantly 'value'. The 'value' key is\n",
    "        # the one we want to use as that's the one selected by the algorithm\n",
    "        v = pars['value']\n",
    "        if k == 'beta':\n",
    "            sir.pars.beta = ss.beta(v)\n",
    "        elif k == 'init_prev':\n",
    "            sir.pars.init_prev = ss.bernoulli(v)\n",
    "        elif k == 'n_contacts':\n",
    "            net.pars.n_contacts = ss.poisson(v)\n",
    "        else:\n",
    "            raise NotImplementedError(f'Parameter {k} not recognized')\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Starsim framework has been integrated with the Optuna hyperparameter optimization algorithm to facilitate calibration through the `Calibration` class. Recall that an optimization-based approach to calibration minimizes a function of the input parameters. This function is key to achieving an acceptable calibration.\n",
    "\n",
    "There are two ways to describe the goodness-of-fit function for the `Calibration`. The first method is to directly provide a function that the algorithm will call. The `eval_fn` will be passed each completed `sim` after running, and is expected to return a float representing the goodness of fit (higher is better). Data can be passed into the `eval_fn` via `eval_kwargs`.\n",
    "\n",
    "As an alternative to directly specifying the evaluation function, you can use `CalibComponent`s. Each component includes real data, for example from a survey, that is compared against simulation data from the model. Several components and be used at the same time, for example one for disease prevalence and another for treatment coverage. Each component computes a likelihood of the data given the input parameters, as assess via simulation. Components are combined assuming independence.\n",
    "\n",
    "When defining a `CalibComponent`, we give it a `name` and pass in `expected` (the real data to be calibrated to). The required data fields depend on the likelihood function. Importantly, the functional form of the negative log likelihood, or nll, is defined by the `nll_fn`. The value for `nll_fn` can be `'beta'`, `'gamma'`, or a negative log likelihood function of your own creation. If designing your own function for `nll_fn`, it should take two arguments: `expected` and `actual`. For a beta binomial, the data must define `n` and `x`, where `n` is the number of individuals who were sampled and `x` is the number that were found, e.g. identified as positive.\n",
    "\n",
    "Output from the simulation is obtained via a function. The function takes a completed `sim` object as input and returns a dictionary with fields as required for the evaluation function of your choice. In the example below, we use an in-line lambda function to extract `n` and `x` from the simulation, as required by the Beta binomial component.\n",
    "\n",
    "Each component has a `weight`. The final goodness of fit is a weighted sum of negative log likelihoods.\n",
    "\n",
    "Finally, the `conform` argument describes how the simulation output is adjusted to align with the real data. For example, if the real data is a prevalence measurement, choosing `'prevalent'` will interpolate the simulation output at the time points of the real data. Choosing `'incident'`, the simulation output will be aggregated between time points of the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectious = ss.CalibComponent(\n",
    "    name = 'Infectious',\n",
    "\n",
    "    # For this example, the \"expected\" comes from a simulation with pars\n",
    "    #   beta=0.075, init_prev=0.02, n_contacts=4\n",
    "    expected = pd.DataFrame({\n",
    "        'n': [200, 197, 195], # Number of individuals sampled\n",
    "        'x': [30, 30, 10],    # Number of individuals found to be infectious\n",
    "    }, index=pd.Index([ss.date(d) for d in ['1990-01-12', '1990-01-25', '1990-02-02']], name='t')), # On these dates\n",
    "\n",
    "    extract_fn = lambda sim: pd.DataFrame({\n",
    "        'n': sim.results.n_alive, # Number of individuals sampled\n",
    "        'x': sim.results.sir.n_infected, # Number of individuals found to be infectious\n",
    "    }, index=pd.Index(sim.results.timevec, name='t')), # Index is time\n",
    "\n",
    "    conform = 'prevalent',\n",
    "    nll_fn = 'beta',\n",
    "\n",
    "    weight = 1, # Not required if only one component\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can bring all the pieces together. We make a single base simulation and create an instance of a Starsim Calibration object. This object requires a few arguments, like the `calib_pars` and `sim`. We also pass in the function that modifies the base `sim`, here our `build_sim` function. No additional `build_kw` are required in this example.\n",
    "\n",
    "We also pass in a list of `components`. Instead of using this \"component-based\" system, a user could simply provide an `eval_fn`, which takes in a completed sim an any `eval_kwargs` and returns a \"goodness of fit\" score to be maximized.\n",
    "\n",
    "We can also specify the total number of trial to run, the number of parallel works, and a few other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.heading('Beginning calibration')\n",
    "\n",
    "# Make the sim and data\n",
    "sim = make_sim()\n",
    "\n",
    "# Make the calibration\n",
    "calib = ss.Calibration(\n",
    "    calib_pars = calib_pars,\n",
    "    sim = sim,\n",
    "\n",
    "    build_fn = build_sim, # Use default builder, Calibration.translate_pars\n",
    "    build_kw = None,\n",
    "\n",
    "    components = [infectious],\n",
    "\n",
    "    total_trials = 100,\n",
    "    n_workers = None, # None indicates to use all available CPUs\n",
    "    die = True,\n",
    "    debug = debug,\n",
    ")\n",
    "\n",
    "# Perform the calibration\n",
    "sc.printcyan('\\nPeforming calibration...')\n",
    "calib.calibrate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the best parameters that were found. Note that the `rand_seed` was selected at random, but the other parameters are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.best_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the calibration is complete, we can compare the `guess` values to the best values found by calling `check_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm\n",
    "sc.printcyan('\\nConfirming fit...')\n",
    "calib.check_fit(n_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view some plots of the results. Blue is before calibration using the `guess` values whereas orange is after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.plot_trend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
