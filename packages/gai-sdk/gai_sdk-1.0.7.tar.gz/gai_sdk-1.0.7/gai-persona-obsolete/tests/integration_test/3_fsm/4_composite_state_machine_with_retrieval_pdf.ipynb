{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent State Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARRANGE:** index the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[45m\u001b[30mDEBUG   \u001b[0m \u001b[35mhttppost:url=http://localhost:12036/gen/v1/rag/index-file\u001b[0m\n",
      "\u001b[45m\u001b[30mDEBUG   \u001b[0m \u001b[35mhttppost:data=None\u001b[0m\n",
      "\u001b[45m\u001b[30mDEBUG   \u001b[0m \u001b[35mhttppost:files={'file': ('ReAct-2210.03629.pdf',\n",
      "          <_io.BufferedReader name='ReAct-2210.03629.pdf'>,\n",
      "          'application/pdf'),\n",
      " 'req': (None,\n",
      "         '{\"Id\":null,\"CollectionName\":\"demo\",\"FilePath\":\"ReAct-2210.03629.pdf\",\"FileType\":\"\",\"Source\":\"\",\"ByteSize\":null,\"Title\":\"\",\"Abstract\":null,\"Authors\":\"\",\"Publisher\":\"\",\"PublishedDate\":\"\",\"Comments\":\"\",\"Keywords\":\"\",\"ChunkGroupId\":null,\"ChunkSize\":null,\"ChunkOverlap\":null,\"ChunkCount\":null}',\n",
      "         'application/json')}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IndexedDocChunkIdsPydantic(DocumentId='Mm1GAmuGfs3VoA2sit0njabQGAZ75eyfszIkAxGY5e4', ChunkgroupId='18f38de6-ff61-40f8-8645-d17f2dcac0d9', ChunkIds=['40bbea28-2a68-4287-85da-55a43cacdfa4', '6137b0cb-1c92-47f3-98fd-e685adcc2eff', '5cccd669-ecad-4ee3-a1eb-cb00115c861c', '9625a33a-05c1-4237-a3c4-3b4d5783b488', '185d86bf-0619-4ac7-9ef9-43f99412b3a9', '04b23c65-3f6e-486c-88a3-1ff6ee10da29', '6a93fb92-ec84-4fbf-a129-ed77680219f4', '95bde36c-000f-41ea-9194-9200ccb7763c', 'd9917642-7e3c-4565-b083-96a692f27caa', 'ed648273-e080-467c-a96c-d22235eb237e', 'd777051f-7997-4bb3-8497-eebba916c117', 'e14cd0ed-80db-4328-89a7-14ce3f310f8c', '8d74db46-aac1-4367-a302-8bdec7f76927', '4ec5891d-cc15-4ff8-9fad-d0018bc847b2', '9d931c3b-a53d-40ec-8c6f-eb0d010451a2', '9689ef93-6e6c-4c0a-814e-e894606f1103', '2647ad5d-bd27-4ed0-8e25-2848f6f34dbc', 'bac326d2-82e2-461c-9251-c26fac138c51', '9e8d7037-0bb1-4f84-9024-d647c54077ac', 'c0974724-a386-40be-8e4e-bb659dd44518', 'fb99f2b2-b364-479a-99d4-d60d4a579a8a', 'd031305b-18ee-480a-9b8e-37d1f4618b2b', 'bb8b175b-dffe-410d-87ea-eccd35eaa0cd', '44a17f92-3d94-4f36-abfd-9829f2525d9d', '3cba0e34-728b-464c-831d-e8bb42665ee4', 'ab132c81-d638-46d6-9d88-e7b658dd9a5e', '2c89b6a7-3491-4653-a7ae-6327b6c764fd', 'a26089be-3e21-473d-b0c3-9395d15997e0', 'fbdb1fb6-e4c3-4bbf-84a1-38cf9a1c8d6a', '863205df-c4a5-431c-8452-5f9057e41824', 'a82476b9-cdd4-46f4-87ad-015d9da79ab0', 'c446368f-34fd-4754-bd03-8c703dd5a01a', 'b5dbf97c-77de-4b88-87a2-62007d158464', '1efb7206-dec8-41bb-b3a1-9af63659181e', '2d2f6fcb-1570-460e-8846-562f23a872fd', 'e544f1de-56e9-407d-94b0-ef9f21761acc', 'd5be60dd-1bc3-415c-8082-35bee6bd76aa', '9a3acbc2-de4f-49a4-b40e-8657d8d66f0d', 'ea5c810f-bd22-4437-90da-d311f2cc9c71', 'd6c8cab6-5f15-4957-a8d4-5bbd46ab6b7a', '129d675d-4131-425c-9f09-dd5786b9e2f6', 'bbf374a3-c8eb-49ad-ad97-e22f1fe821ab', 'cf416d36-e441-4d46-b818-6f34d6804831', '58b2a774-020d-4fb2-a27c-1e5f5c42bffa', '6f202137-c635-4467-88ee-4874ead9b5b0', '4299e9d4-01a7-400c-8510-3840eaf8f402', '58d1cb4c-27bc-4ed1-a75b-ff8194da3fc9', '97820887-35e6-42ce-8181-770999ffa319', 'b4a5732c-9b76-4421-b9e9-7b6b12f9d7db', '2b714188-c457-47bf-a1e2-09df611496c8', '0b130111-3790-49a3-9e5f-3c9e28f40c25', '8645f320-b9f8-4c5a-a86b-0e374e89f715', '18894254-591b-4cf8-8e2f-ad319a75ede5', 'd4bf255a-86aa-4bc5-b3a7-248ca477eca2', '77bbabc4-3706-42c5-94b6-beaea9705bb7', 'd342482d-50c4-4dd8-b5d8-1e0b8d20422c', '2a33869c-acae-49e0-a043-2f853a1cf7e6', 'cd2ca9b4-95c6-4a68-b919-daf1b77914a3', '5cc131f3-84de-43bf-be88-4319fa784328', 'c3f97f21-1237-44a3-a770-913a263c9fb5', '051ea416-4707-4339-96ca-e32b5f50bbdd', 'b4411885-739c-4efc-a361-0af6be81ea65', 'f7e1fd90-0327-40ce-bf11-b2e5d14f229d', '5e98d2e3-fb6e-4f5d-acb0-32783929f22c', '055e0434-e6cb-47d7-8838-9b519b964c18', '22d5112b-eef7-4ece-88fc-91613a4b662c', '30e8a106-440b-4130-a744-99b4e38ac7d8', 'd6ee507d-b203-402e-b88c-df0d5f7ed135', '63c0bee7-2c69-4d61-9e76-2a703b8c02a7', '3f8a2bb9-f66a-4f30-bfbc-bb57dd8bb357', '1f421c0a-e8eb-42fa-9146-804e19596a66', 'f8a02596-c1d6-4f91-b22b-9943283be2ec', '9ebb2ef3-1770-4588-a559-d55070619dbe', 'a946adf7-87d3-468f-9388-a7cfdecb3692', 'ab53ce1f-79ea-476c-ac90-d0b8e0fcff0d', '28e4860c-eef3-4349-9a00-6feb26ce1b24', '417e188b-f3dd-43ee-9776-a6bd30834052', 'b87affe0-9154-4b5c-96d3-1b20fab61535', '35ffb847-ce61-400a-9e0e-e1659ba021ae', '26b5fd86-8ea9-4077-a217-b68e16a69309', 'a1fa5566-0d98-43b0-9447-b0cd7aa623c5', 'eb105460-0e52-494a-bde4-0e1e2f337132', 'e4c12c9d-9c7b-46e8-bce1-785b5a97e060', '435216e7-acb0-4ef4-a058-2f37c391acad', '5f8dfca0-8d00-449d-bf02-976f9dfbe0b8', '27da5568-626d-4f10-a27a-287f47080b86', '4e05580f-832e-4716-8649-6445a9d03021', '8e3f7aa5-f1c0-48b7-8485-b2a21419851b', '21eabda9-8851-41dc-9813-51acc185bc88', 'e6b2c5f6-9ad8-4e1a-827b-358e21393854', 'db46f378-ca88-4b33-8b6f-814d1aa7b567', '0f288a9a-67b2-4e96-97b2-9c10cb938e40', '89843ab2-8b77-49c7-9b99-a19ced9b7528', '2022752d-691c-4ff0-b5e1-9711a668bdd3', 'c930378e-d91a-47c3-be31-a9a4e00eeb9d', '78563d44-94b7-407e-a815-243311ac840a', '2ba367dd-6a9d-4768-82af-8bd6098ceb7f', '091e7a93-a14a-4d20-9bdc-f7d8fa2867d5', '6834d920-ece0-486c-83f2-f099ec164ca9', '91e73822-631d-4e4a-96c1-934a6e7ea000', '5047819c-e134-40a1-9eb9-a97df2920963', '60088991-7218-4300-8b37-2f22ecb54819', '83f31066-a534-4eae-8dbe-93ca73a177c3', '8d130824-cc39-4b7e-a37e-6f74a8fba062', '813fe094-4205-46a7-9808-13929a7b1ee1', '0c87e46e-658c-4de1-8252-bc30e22cda68', 'b0c4c36a-298f-4b49-a206-44821efeb4c2', '1a1931a3-7cc4-463a-814b-a04c26901270', 'e57d2cfc-1d3a-41d6-b159-c72b1f8e6f40', 'dd352bf1-4d88-45cf-bab2-d18ebb66e31f', '61f41dff-63ce-4c2f-9a12-490091c4c7a2', 'be00098a-99fc-4dd8-931f-f0a8780b596b', '4b3e02c3-1675-43ac-9291-7627e76dc470', '2e501b91-815f-4e9d-881d-d8a0902b55cd'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index test doc\n",
    "import os\n",
    "os.environ[\"LOG_LEVEL\"]=\"DEBUG\"\n",
    "\n",
    "from gai.rag.client.rag_client_async import RagClientAsync\n",
    "rag = RagClientAsync({\n",
    "    \"type\": \"rag\",\n",
    "    \"url\": \"http://localhost:12036/gen/v1/rag\",\n",
    "    \"ws_url\": \"ws://localhost:12036/gen/v1/rag/index-file/ws\"\n",
    "})\n",
    "here = os.path.dirname(__name__)\n",
    "file_path = os.path.join(here,\"ReAct-2210.03629.pdf\")\n",
    "await rag.index_document_async(collection_name=\"demo\", file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 0. Init\n",
    "\n",
    "The starting point is to create the string representation of the following mermaid diagram as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "stateDiagram-v2\n",
    "    INIT --> TOOL_CHOICE: next / on_TOOL_CHOICE\n",
    "\n",
    "    TOOL_CHOICE --> CRAFT_TEXT_PROMPT: next / on_CRAFT_PROMPT / use_text\n",
    "        CRAFT_TEXT_PROMPT --> GENERATE: next / on_GENERATE\n",
    "        GENERATE --> PROCESS: next / on_PROCESS / use_text\n",
    "        PROCESS --> END : next\n",
    "\n",
    "    TOOL_CHOICE --> CRAFT_TOOL_PROMPT: next / on_CRAFT_PROMPT / use_google\n",
    "        TOOL_CALL --> GOOGLE: next / on_GOOGLE / use_google\n",
    "        GOOGLE --> GENERATE: next / on_GENERATE\n",
    "\n",
    "    TOOL_CHOICE --> CRAFT_TOOL_PROMPT: next / on_CRAFT_PROMPT / use_retrieval\n",
    "        TOOL_CALL --> RETRIEVAL: next / on_RETRIEVAL / use_retrieval\n",
    "        RETRIEVAL --> GENERATE: next / on_GENERATE\n",
    "\n",
    "    CRAFT_TOOL_PROMPT --> TOOL_CALL: next / on_TOOL_CALL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine validation successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LOG_LEVEL\"]=\"WARNING\"\n",
    "from gai.rag.client.rag_client_async import RagClientAsync\n",
    "from gai.persona.profile.pydantic.AgentPydantic import AgentPydantic\n",
    "from gai.ttt.client.ttt_client import TTTClient\n",
    "from gai.persona.fsm.AgentStateMachine import AgentStateMachine\n",
    "from gai.lib.common.notebook import highlight,print_colored\n",
    "\n",
    "state_diagram = \"\"\"stateDiagram-v2\n",
    "    INIT --> TOOL_CHOICE: next / on_TOOL_CHOICE\n",
    "\n",
    "    TOOL_CHOICE --> CRAFT_TEXT_PROMPT: next / on_CRAFT_PROMPT / use_text\n",
    "        CRAFT_TEXT_PROMPT --> GENERATE: next / on_GENERATE\n",
    "        GENERATE --> PROCESS: next / on_PROCESS / use_text\n",
    "        PROCESS --> END : next\n",
    "\n",
    "    TOOL_CHOICE --> CRAFT_TOOL_PROMPT: next / on_CRAFT_PROMPT / use_google\n",
    "        TOOL_CALL --> GOOGLE: next / on_GOOGLE / use_google\n",
    "        GOOGLE --> GENERATE: next / on_GENERATE\n",
    "\n",
    "    TOOL_CHOICE --> CRAFT_TOOL_PROMPT: next / on_CRAFT_PROMPT / use_retrieval\n",
    "        TOOL_CALL --> RETRIEVAL: next / on_RETRIEVAL / use_retrieval\n",
    "        RETRIEVAL --> GENERATE: next / on_GENERATE\n",
    "\n",
    "    CRAFT_TOOL_PROMPT --> TOOL_CALL: next / on_TOOL_CALL\n",
    "    \"\"\"\n",
    "\n",
    "ttt = TTTClient({\n",
    "    \"type\": \"ttt\",\n",
    "    \"url\": \"http://localhost:12031/gen/v1/chat/completions\",\n",
    "    \"timeout\": 60.0,\n",
    "    \"temperature\":10e-9,\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"max_tokens\": 2000,\n",
    "})\n",
    "\n",
    "rag = RagClientAsync({\n",
    "    \"type\": \"rag\",\n",
    "    \"url\": \"http://localhost:12036/gen/v1/rag\",\n",
    "    \"ws_url\": \"ws://localhost:12036/gen/v1/rag/index-file/ws\"\n",
    "})\n",
    "\n",
    "agent_data = AgentPydantic(\n",
    "    Id=\"\",\n",
    "    Name=\"Alfred\",\n",
    "    AgentDescription=\"Hello, I am Alfred, the best large language model in the world, and I am here to assist you in any way possible. As a highly advanced AI assistant, I possess the ability to perform general-purpose tasks powered by <span ${style}>GPT-4</span>. With my vast knowledge base and powerful processing capabilities, I am able to provide you with the most relevant and helpful information available. Whether you need answers to complex questions, recommendations for products or services, or assistance with decision making, I am here to help. So, how may I be of service to you today?\",\n",
    "    AgentTraits=\"Wise,Serious,Meticulous\",\n",
    "    ImageUrl=\"\",\n",
    "    ThumbnailUrl=\"\"\n",
    "    )\n",
    "doc_titles=[\"REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\"]\n",
    "doc_keywords=[\"AI\",\"Reasoning\",\"LLM\"]\n",
    "tools_config={\n",
    "    \"google\": {\n",
    "        \"tool_prompt\":\"👩‍🔬, use only the information provided to you by the user to answer the user''s question.\\n            👩‍🔬, whenever possible, do not simply answer the question but try to be as informative as you can.\\n            Remember, these information are scraped from the web so you may need to proofread and edit the content before responding.\\n            👩‍🔬 will reply in point forms, precede each point with a newline \\\"\\n\\\", and be precise in your articulation.\\n            👩‍🔬 will provide your own reasoned subjective perspective, noting where your view differs from or expands on the contents.\\n            Rules:\\n                - Consolidate the materials provided by the user and then organise them point by point.\\n                - Don't just answer the question, be as informative as you can. For example, provide and proofread some background information or fun-fact to support your answer and make it interesting.\\n                - Begin your report by saying `According to my online research,...`\\n                - Always provide your answers in point form.\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"google\",\n",
    "                \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "                \"arguments\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }                    \n",
    "        }\n",
    "    },\n",
    "    \"retrieval\": {\n",
    "        \"tool_prompt\":\"👩‍🔬, use only the information provided to you by the user to answer the user''s question. \\n            If the information is insufficient for 👩‍🔬 to derive an answer, just say ''I cannot find relevant information in my document store to answer the question correctly.'' \\n            👩‍🔬 is to provide an in-depth analysis to the question based only on the information provided by the user and nothing more.\\n            👩‍🔬 will give a real-life example to support illustrating your point and contrasting it with a counter-example. \\n            👩‍🔬 will also proofread and edit the content before responding. \\n            👩‍🔬 will provide your own reasoned subjective perspective, noting where your view differs from or expands on the contents.\\n            Rules:\\n                - Consolidate the materials provided by the user and then organise them point by point.\\n                - Provide as much details as you can extract from the materials provided by the user.\\n                - Begin your report by saying `According to my document store,...`\\n                - Always provide your answers in point form.\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"retrieval\",\n",
    "                \"description\": f\"\"\"\n",
    "                    The `retrieval` function is a powerful tool that allows the AI to access articles outside of its knowledge domain from external sources. \n",
    "                    The external articles are stored in an archive and organised by <titles>:\\n{{ titles: [{doc_titles}] }}\n",
    "                    and <keywords>:\n",
    "                    {{ keywords: [{doc_keywords}] }}\n",
    "                    **IMPORTANT**: Use this tool when any of the <titles> or <keywords> may be relevant to user's question.\n",
    "                    The \\'search_query\\' parameter should be crafted in a way that it returns the most precise result based on the conversation context.\n",
    "                \"\"\",\n",
    "                \"arguments\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"search_query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"\"\"The most effective search query for semantic search that will return the most precise result.\"\"\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"search_query\"]\n",
    "                }\n",
    "            }                    \n",
    "        }\n",
    "    },\n",
    "    \"text\":{\n",
    "        \"tool_prompt\":\"\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"text\",\n",
    "                \"description\": \"The 'text' function is the default catch-all function returned when none of the other tools are applicable.\",\n",
    "                \"arguments\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"message\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The user's message.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"message\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "user_message=\"How does REACT differ from Chain-of-thought?\"\n",
    "agent = AgentStateMachine(\n",
    "    ttt=ttt,\n",
    "    rag=rag,\n",
    "    agent_data=agent_data,\n",
    "    collection_name=\"demo\",\n",
    "    dialogue_messages=[],\n",
    "    user_message=user_message,\n",
    "    tools_config=tools_config,\n",
    "    n_search=3,\n",
    "    n_rag=3,\n",
    "    state_diagram=state_diagram\n",
    ").Init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': '',\n",
       " 'Name': 'Alfred',\n",
       " 'Generator': None,\n",
       " 'AgentDescription': 'Hello, I am Alfred, the best large language model in the world, and I am here to assist you in any way possible. As a highly advanced AI assistant, I possess the ability to perform general-purpose tasks powered by <span ${style}>GPT-4</span>. With my vast knowledge base and powerful processing capabilities, I am able to provide you with the most relevant and helpful information available. Whether you need answers to complex questions, recommendations for products or services, or assistance with decision making, I am here to help. So, how may I be of service to you today?',\n",
       " 'ImageUrl': '',\n",
       " 'ThumbnailUrl': '',\n",
       " 'AssociatedUserId': None,\n",
       " 'AgentTraits': 'Wise,Serious,Meticulous',\n",
       " 'AgentVoiceId': None,\n",
       " 'AgentVoiceType': None,\n",
       " 'AgentVoiceName': None,\n",
       " 'UsageType': None,\n",
       " 'AgentSkills': None,\n",
       " 'AgentShortDesc': None,\n",
       " 'AgentHyperparameters': None,\n",
       " 'ClassType': None,\n",
       " 'Tools': None,\n",
       " 'CustomPrompt': None,\n",
       " 'AgentFlow': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent_data.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 1. TOOL_CHOICE\n",
    "\n",
    "The first state is managed by `use_TOOL_CHOICE_handler` state handler function.\n",
    "The agent should decide to use \"retrieval\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1\n",
      "state=TOOL_CHOICE\n",
      "tool_choice=required\n",
      "tool_name='retrieval\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background-color: #cccccc; color: #333333; white-space: pre-wrap; padding: 10px;\">content=<i>{\n",
       "    \"result\": \"retrieval\"\n",
       "}</i></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MONOLOGUE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from TestAgentHelper import TestAgentHelper\n",
    "\n",
    "# ACT\n",
    "agent.next()\n",
    "\n",
    "# ASSERT\n",
    "TestAgentHelper.ShowAgent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 2. CRAFT_TOOL_PROMPT\n",
    "\n",
    "CRAFT_TOOL_PROMPT is triggered because the agent recommends \"retrieval\". System message should return \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=2\n",
      "state=CRAFT_TOOL_PROMPT\n",
      "tool_choice=required\n",
      "tool_name='retrieval\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background-color: #cccccc; color: #333333; white-space: pre-wrap; padding: 10px;\">content=<i>{\n",
       "    \"system_message\": \"\"\n",
       "}</i></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MONOLOGUE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">System:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">User:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred, How does REACT differ from Chain-of-thought?\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Assistant:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ACT\n",
    "agent.next()\n",
    "\n",
    "# ASSERT\n",
    "TestAgentHelper.ShowAgent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 3. TOOL_CALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"function\", \"name\": \"retrieval\", \"arguments\": \"{\\\"search_query\\\": \\\"REACT vs Chain-of-thought\\\"}\"}\n",
      "step=3\n",
      "state=TOOL_CALL\n",
      "tool_choice=required\n",
      "tool_name='retrieval\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background-color: #cccccc; color: #333333; white-space: pre-wrap; padding: 10px;\">content=<i>\"{\\\"type\\\": \\\"function\\\", \\\"name\\\": \\\"retrieval\\\", \\\"arguments\\\": \\\"{\\\\\\\"search_query\\\\\\\": \\\\\\\"REACT vs Chain-of-thought\\\\\\\"}\\\"}\"</i></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MONOLOGUE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">System:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">User:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred, How does REACT differ from Chain-of-thought?\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Assistant:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ACT\n",
    "agent.next()\n",
    "print(agent.content)\n",
    "\n",
    "# ASSERT\n",
    "TestAgentHelper.ShowAgent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 4. RETRIEVAL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=4\n",
      "state=RETRIEVAL\n",
      "tool_choice=none\n",
      "tool_name='text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background-color: #cccccc; color: #333333; white-space: pre-wrap; padding: 10px;\">content=<i>[\n",
       "    \"within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous \\u201cinner monologue\\u201d. However, IM\\u2019s \\u201cinner monologue\\u201d is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satis\\ufb01ed. In contrast, the reasoning traces in ReAct for decision making is \\ufb02exible and sparse, allowing diverse reasoning types (see Section 2) to be induced for different tasks. To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on \\ufb01ve out of six tasks. Qualitatively, we observed that ReAct-IM often\",\n",
       "    \"on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic bene\\ufb01ts compared to reasoning or acting alone. In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason). 2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018), fact veri\\ufb01cation (Fever, Thorne et al., 2018),\",\n",
       "    \"example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3 Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself. Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators\",\n",
       "    \"synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veri\\ufb01cation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success\"\n",
       "]</i></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MONOLOGUE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">System:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👩‍🔬, use only the information provided to you by the user to answer the user''s question. \n",
      "            If the information is insufficient for 👩‍🔬 to derive an answer, just say ''I cannot find relevant information in my document store to answer the question correctly.'' \n",
      "            👩‍🔬 is to provide an in-depth analysis to the question based only on the information provided by the user and nothing more.\n",
      "            👩‍🔬 will give a real-life example to support illustrating your point and contrasting it with a counter-example. \n",
      "            👩‍🔬 will also proofread and edit the content before responding. \n",
      "            👩‍🔬 will provide your own reasoned subjective perspective, noting where your view differs from or expands on the contents.\n",
      "            Rules:\n",
      "                - Consolidate the materials provided by the user and then organise them point by point.\n",
      "                - Provide as much details as you can extract from the materials provided by the user.\n",
      "                - Begin your report by saying `According to my document store,...`\n",
      "                - Always provide your answers in point form.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">User:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Refer to the following context: `['within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However, IM’s “inner monologue” is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the reasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning types (see Section 2) to be induced for different tasks. To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often', 'on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneﬁts compared to reasoning or acting alone. In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason). 2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),', 'example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3 Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself. Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators', 'synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success']`. \n",
      "            Based on the earlier context, answer the question How does REACT differ from Chain-of-thought?.\"\n",
      "            \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Assistant:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ACT\n",
    "agent.next()\n",
    "\n",
    "# ASSERT\n",
    "TestAgentHelper.ShowAgent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 5. GENERATE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to my document store, ReAct and Chain-of-thought (CoT) differ in several ways. Firstly, ReAct is a general paradigm that combines reasoning and acting with language models for solving diverse language reasoning and decision making tasks. In contrast, CoT is a method used for solving complex reasoning tasks by having a model generate a series of thoughts or steps to arrive at an answer.\n",
      "\n",
      "Secondly, ReAct prompts language models to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with external environments to incorporate additional information into reasoning (act to reason). On the other hand, CoT does not explicitly involve actions, but rather focuses on the generation of thoughts or steps to solve a problem.\n",
      "\n",
      "Thirdly, ReAct addresses issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. However, it is not mentioned in the provided context whether CoT has similar mechanisms to address these issues.\n",
      "\n",
      "Lastly, ReAct is demonstrated to outperform CoT-like methods (ReAct-IM) in empirical evaluations on diverse benchmarks. Specifically, ReAct substantially outperforms IM-style prompting (ReAct-IM) in terms of overall success rate, with consistent advantages on five out of six tasks.\n",
      "\n",
      "In summary, while both ReAct and CoT are methods used for solving complex reasoning tasks, they differ in their approach, integration with actions, and mechanisms to address issues of hallucination and error propagation."
     ]
    }
   ],
   "source": [
    "# ACT\n",
    "agent.next()\n",
    "for chunk in agent.streamer:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=5\n",
      "state=GENERATE\n",
      "tool_choice=none\n",
      "tool_name='text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background-color: #cccccc; color: #333333; white-space: pre-wrap; padding: 10px;\">content=<i>\" According to my document store, ReAct and Chain-of-thought (CoT) differ in several ways. Firstly, ReAct is a general paradigm that combines reasoning and acting with language models for solving diverse language reasoning and decision making tasks. In contrast, CoT is a method used for solving complex reasoning tasks by having a model generate a series of thoughts or steps to arrive at an answer.\\n\\nSecondly, ReAct prompts language models to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with external environments to incorporate additional information into reasoning (act to reason). On the other hand, CoT does not explicitly involve actions, but rather focuses on the generation of thoughts or steps to solve a problem.\\n\\nThirdly, ReAct addresses issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. However, it is not mentioned in the provided context whether CoT has similar mechanisms to address these issues.\\n\\nLastly, ReAct is demonstrated to outperform CoT-like methods (ReAct-IM) in empirical evaluations on diverse benchmarks. Specifically, ReAct substantially outperforms IM-style prompting (ReAct-IM) in terms of overall success rate, with consistent advantages on five out of six tasks.\\n\\nIn summary, while both ReAct and CoT are methods used for solving complex reasoning tasks, they differ in their approach, integration with actions, and mechanisms to address issues of hallucination and error propagation.\"</i></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MONOLOGUE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">System:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👩‍🔬, use only the information provided to you by the user to answer the user''s question. \n",
      "            If the information is insufficient for 👩‍🔬 to derive an answer, just say ''I cannot find relevant information in my document store to answer the question correctly.'' \n",
      "            👩‍🔬 is to provide an in-depth analysis to the question based only on the information provided by the user and nothing more.\n",
      "            👩‍🔬 will give a real-life example to support illustrating your point and contrasting it with a counter-example. \n",
      "            👩‍🔬 will also proofread and edit the content before responding. \n",
      "            👩‍🔬 will provide your own reasoned subjective perspective, noting where your view differs from or expands on the contents.\n",
      "            Rules:\n",
      "                - Consolidate the materials provided by the user and then organise them point by point.\n",
      "                - Provide as much details as you can extract from the materials provided by the user.\n",
      "                - Begin your report by saying `According to my document store,...`\n",
      "                - Always provide your answers in point form.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">User:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Refer to the following context: `['within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However, IM’s “inner monologue” is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the reasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning types (see Section 2) to be induced for different tasks. To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often', 'on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneﬁts compared to reasoning or acting alone. In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason). 2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),', 'example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3 Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself. Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators', 'synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success']`. \n",
      "            Based on the earlier context, answer the question How does REACT differ from Chain-of-thought?.\"\n",
      "            \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Assistant:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ASSERT\n",
    "TestAgentHelper.ShowAgent(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State 6. PROCESS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=6\n",
      "state=PROCESS\n",
      "tool_choice=none\n",
      "tool_name='text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"background-color: #cccccc; color: #333333; white-space: pre-wrap; padding: 10px;\">content=<i>\" According to my document store, ReAct and Chain-of-thought (CoT) differ in several ways. Firstly, ReAct is a general paradigm that combines reasoning and acting with language models for solving diverse language reasoning and decision making tasks. In contrast, CoT is a method used for solving complex reasoning tasks by having a model generate a series of thoughts or steps to arrive at an answer.\\n\\nSecondly, ReAct prompts language models to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with external environments to incorporate additional information into reasoning (act to reason). On the other hand, CoT does not explicitly involve actions, but rather focuses on the generation of thoughts or steps to solve a problem.\\n\\nThirdly, ReAct addresses issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. However, it is not mentioned in the provided context whether CoT has similar mechanisms to address these issues.\\n\\nLastly, ReAct is demonstrated to outperform CoT-like methods (ReAct-IM) in empirical evaluations on diverse benchmarks. Specifically, ReAct substantially outperforms IM-style prompting (ReAct-IM) in terms of overall success rate, with consistent advantages on five out of six tasks.\\n\\nIn summary, while both ReAct and CoT are methods used for solving complex reasoning tasks, they differ in their approach, integration with actions, and mechanisms to address issues of hallucination and error propagation.\"</i></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MONOLOGUE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">System:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👩‍🔬, use only the information provided to you by the user to answer the user''s question. \n",
      "            If the information is insufficient for 👩‍🔬 to derive an answer, just say ''I cannot find relevant information in my document store to answer the question correctly.'' \n",
      "            👩‍🔬 is to provide an in-depth analysis to the question based only on the information provided by the user and nothing more.\n",
      "            👩‍🔬 will give a real-life example to support illustrating your point and contrasting it with a counter-example. \n",
      "            👩‍🔬 will also proofread and edit the content before responding. \n",
      "            👩‍🔬 will provide your own reasoned subjective perspective, noting where your view differs from or expands on the contents.\n",
      "            Rules:\n",
      "                - Consolidate the materials provided by the user and then organise them point by point.\n",
      "                - Provide as much details as you can extract from the materials provided by the user.\n",
      "                - Begin your report by saying `According to my document store,...`\n",
      "                - Always provide your answers in point form.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">User:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Refer to the following context: `['within a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However, IM’s “inner monologue” is limited to observations of the environment state and what needs to be completed by the agent for the goal to be satisﬁed. In contrast, the reasoning traces in ReAct for decision making is ﬂexible and sparse, allowing diverse reasoning types (see Section 2) to be induced for different tasks. To demonstrate the differences between ReAct and IM, and to highlight the importance of internal reasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought pattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially outperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent advantages on ﬁve out of six tasks. Qualitatively, we observed that ReAct-IM often', 'on how reasoning and acting can be combined in a synergistic manner for general task solving, and if such a combination can bring systematic beneﬁts compared to reasoning or acting alone. In this work, we present ReAct, a general paradigm to combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks (Figure 1). ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason). 2 Published as a conference paper at ICLR 2023 We conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks: question answering (HotPotQA, Yang et al., 2018), fact veriﬁcation (Fever, Thorne et al., 2018),', 'example is a human trajectory of actions, thoughts, and environment observations to solve a task instance (see Appendix C). For the tasks where reasoning is of primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the task-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision making tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to 1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B. 3 Published as a conference paper at ICLR 2023 appear sparsely in the most relevant positions of a trajectory, so we let the language model decide the asynchronous occurrence of thoughts and actions for itself. Since decision making and reasoning capabilities are integrated into a large language model, ReAct enjoys several unique features: A) Intuitive and easy to design: Designing ReAct prompts is straightforward as human annotators', 'synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success']`. \n",
      "            Based on the earlier context, answer the question How does REACT differ from Chain-of-thought?.\"\n",
      "            \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"yellow\">Alfred:\n",
       "\n",
       "</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to my document store, ReAct and Chain-of-thought (CoT) differ in several ways. Firstly, ReAct is a general paradigm that combines reasoning and acting with language models for solving diverse language reasoning and decision making tasks. In contrast, CoT is a method used for solving complex reasoning tasks by having a model generate a series of thoughts or steps to arrive at an answer.\n",
      "\n",
      "Secondly, ReAct prompts language models to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interacting with external environments to incorporate additional information into reasoning (act to reason). On the other hand, CoT does not explicitly involve actions, but rather focuses on the generation of thoughts or steps to solve a problem.\n",
      "\n",
      "Thirdly, ReAct addresses issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. However, it is not mentioned in the provided context whether CoT has similar mechanisms to address these issues.\n",
      "\n",
      "Lastly, ReAct is demonstrated to outperform CoT-like methods (ReAct-IM) in empirical evaluations on diverse benchmarks. Specifically, ReAct substantially outperforms IM-style prompting (ReAct-IM) in terms of overall success rate, with consistent advantages on five out of six tasks.\n",
      "\n",
      "In summary, while both ReAct and CoT are methods used for solving complex reasoning tasks, they differ in their approach, integration with actions, and mechanisms to address issues of hallucination and error propagation.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ACT\n",
    "agent.next()\n",
    "\n",
    "# ASSERT\n",
    "TestAgentHelper.ShowAgent(agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
